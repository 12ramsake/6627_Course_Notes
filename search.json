[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes for MATH 6627: Statistical Consulting Practicum",
    "section": "",
    "text": "Preface\n\nThese notes are to be used for MATH 6627: Practicum in Statistical Consulting\nSome references used are:\nLastly, see below:\n\n\nprint(\"Make sure you install R!\")\n\n[1] \"Make sure you install R!\""
  },
  {
    "objectID": "mixed_models.html#clustered-data",
    "href": "mixed_models.html#clustered-data",
    "title": "1  Mixed Models",
    "section": "1.1 Clustered data",
    "text": "1.1 Clustered data\n\n1.1.1 Review of multiple linear regression\nRecall the multiple linear regression model:\nFor the model \\(Y|X=X\\beta+\\epsilon\\), we have\n\n\\(Y\\in \\mathbb{R}^{n\\times 1}\\) is the response variable (a continuous random vector)\n\\(X\\in \\mathbb{R}^{n\\times p}\\) is the covariate matrix (Note that the first column is often \\(1_n\\) – the column vector of ones)\n\\(X_i\\in \\mathbb{R}^{p\\times 1}\\) is the \\(i^{th}\\) observed explanatory variable \\((i=1, \\ldots, n)\\) (not a random variable, in the sense that we condition on it)\n\\(\\beta\\in \\mathbb{R}^{p\\times 1}\\) is the coefficient vector\n\\(\\epsilon\\in\\mathbb{R}^n\\) is the random error (continuous random variable)\n\nThe key assumptions of the (normal) MLR are that\n\n\\(\\epsilon\\) is multivariate normally distributed\n\\(E(\\epsilon)=0\\)\n\\(Cov(\\epsilon)=\\sigma^2 I_n\\)\n\\(Y|X=X\\beta+\\epsilon\\)\n\nAs such, it is critical that when applying MLR models, the observations are independent. However, there are many, many problems where the data contains dependent observations. If we have data that can be split into mutually independent clusters, then we call this clustered data.\n\n\n1.1.2 An example\nConsider the following simple example. Suppose a study wishes to prove/disprove the following: Does Ozempic cause sustained weight loss over time?\nWhat type of data would we need to answer this question? We might start with the question: Can we collect data that would allow us to answer this question with a MLR model?\nCould we:\n\nTake a sample of individuals on Ozempic and measure their weight? – No. How do we determine if their weight has decreased since starting it?\nTake a sample of individuals both on and not on Ozempic at a point in time, and compare their weights? – No. How can we rule out the fact that these are different populations?\n\nIt seems that this question could not be reliably answered using the above suggested methods. We would need to be able to follow individuals, starting when they begin Ozempic, recording their weights, and continue following them for a period of time. We might have data that looks like:\n\n\n\nMonth 1\nMonth 2\nMonth 3\nMonth 4\n…\n\n\n\n\n360\n355\n350\n340\n…\n\n\n225\n222\n224\n225\n…\n\n\n288\n270\n253\n260\n…\n\n\n\nWe could simply compare the weights in month 1 to the last month measured, and apply a one-sample t-test. What if the patients lose weight in the first 6 months and then gain it back? This would not be captured by such a model. We could run one t-test for each month, but of course then the type-1 error would be very large.\nIt is better to model the weights of patients on Ozempic over time. Inspired by the Normal MLR model, we might posit that patient \\(i\\)s weight at time \\(j\\) is governed by the following equation:\n\\[Y_{ij}=\\beta_0+\\beta_{1}t_j+\\epsilon_{ij}.\\] with \\(\\epsilon_{ij}\\sim\\mathcal{N}(0,\\sigma^2)\\). Now, if we want to apply the Normal MLR model, we would need to assume \\(\\epsilon_{ij}\\perp\\epsilon_{\\ell k}\\) when \\(ij\\neq\\ell k\\) . Is this reasonable? This would implies that \\(Y_{ij}\\perp Y_{ij+1}\\), i.e., a patients weight in month \\(j\\) is independent of their weight in month \\(j+1\\). This is, of course unreasonable. However, it would be reasonable to assume that \\(Y_{ij}\\perp Y_{\\ell k}\\) when \\(\\ell\\neq i\\). This is an example of clustered data. Here the clusters are the patients.\n\nIt is safe to assume that a patient’s weight at a given time is unrelated to another patient’s weight at any given time\nHowever, a patients weight a given time is related to their past and future weights; the within patient weights are dependent.\n\nTherefore, a better assumption might be that the \\[Y_{ij}=\\beta_{0i}+\\beta_{1}t_j+\\epsilon_{ij}.\\]\nwhere \\(\\beta_{0i}\\) are now random variables, where there exists one per patient. The coefficients \\(\\beta_{0i}\\) contain the dependence of the weight within individuals, and allow us to model the random errors as independent: \\(Cov(\\epsilon)=\\sigma^2 I\\). This is only one way to model the within-patient dependence between patients.\n\nExample 1.1 Testing this theory…\nSimplify the correlation between \\(Y_{ik}\\) and \\(Y_{ij}\\) in this model, and in the Normal MLR (the Normal MLR is the MLR, with the added assumption that the errors are normally distributed. Compare the results.\n\n\n\n\n\n\n\nSolution:\nDone in lecture.\n\n\n\nThe previous example is a longitudinal study, which is a sub-type of the more general clustered data. A longitudinal study is a research study in which subjects are followed over time. Typically this involves repeated measurements of the same variables. Longitudinal studies differ from cross-sectional studies and time series studies. Cross-sectional studies have no clusters, and time series follow one ore more variates over time. The random errors in a time series may be correlated.\nLongitudinal studies are useful for:\n\ndetecting changes in outcomes, both at the population and individual level,\nassessing Longitudinal effects, as compared to cohort effects/cross sectional effects,\nunderstanding different sources of variation, e.g., between- and within-subject variation.\ndetecting time effects, both directly and as interactions with other relevant factors.\n\nOne example of longitudinal data, which we will see later is the TLC trial data:\n\n########################################################\nTLC &lt;- read.csv(\"data/TLC.csv\",stringsAsFactors = T)\nhead(TLC)\n\n  ID Treatment   W0   W1   W4   W6\n1  1         P 30.8 26.9 25.8 23.8\n2  2         A 26.5 14.8 19.5 21.0\n3  3         A 25.8 23.0 19.1 23.2\n4  4         P 24.7 24.5 22.0 22.5\n5  5         A 20.4  2.8  3.2  9.4\n6  6         A 20.4  5.4  4.5 11.9\n\n\nAs mentioned, longitudinal data is one example of clustered data. Clustered data refers to data that can be divided into clusters, such that data within a given cluster are correlated. For longitudinal observations, observations taken from the same subject at different time points are correlated because they belong to the same subject. In general, real world data have a complex dependence structure - can often be fit into this clustered framework.\nAnother example of clustered data is hierarchical data. These data have clusters within clusters. You could make a super dated inception joke here. For instance, if we wanted to assess how a new way of teaching p-values affects statistical literacy, we could sample universities, then professors, then classes. Here, assuming professors teach multiple sections, we could safely assume that the effect of this new teaching method may differ by university, by professor, and by class. In other words, observations within these groups would be correlated.\n\n\n1.1.3 Additional challenges with clustered data\nOften, clustered data are accompanied by other, additional challenges.\n\nMissing data or dropouts\nMeasurement errors\nCensoring\nOutliers\n\nThe below examples are adapted from Wu (2019) .\n\nExample 1.2 Blood pressure\nA researcher wishes to evaluate a treatment for reducing high blood pressure. Blood pressures of each subject in the study are measured before and after the treatment. The researcher is also interested in how blood pressures of the subjects change over time after the treatment, so blood pressure is also measured after treatment once a month for 5 months. What are some potential challenges associated with this data? One answer: The data may contain missing values, e.g., drop out. Blood pressure has measurement error – often repeatedly measured.\n\n\nExample 1.3 Mental distress\nInvestigate changes in subjects’ mental distress over time in a treatment group and a control group. Mental distress in 239 subjects were measured at baseline, 4, 12, 24, and 60 months, based on their answers to questionnaires. Subjects randomly assigned into two groups: a treatment and a control group. The Global Severity Index (GSI) is used to measure subjects’ distress levels. Other variables such as education, annual income, depression, anxiety, etc. were collected\n  \nWhat are some potential issues for analysis?\nSubstantial individual variability, Missing data, Outliers, Measurement error? GSI influenced by short-term emotional state\n\n\nExample 1.4 AIDS Study\nThe following is an AIDS study designed to evaluate an anti-HIV treatment. 53 HIV infected patients were treated with an antiviral regimen. Viral load (RNA) was repeatedly quantified on days 0, 2, 7, 10, 14, 21, and 28, and weeks 8, 12, 24, and 48 after initiation of the treatment. Immunologic markers known as CD4 and CD8 cell counts were also measured along with viral load, as well as some other variables. Viral load has a lower detection limit of 100, i.e., viral loads below 100 are not quantifiable.\n\n\n\nAD1\n\n\n\n\n\nAD2\n\n\n\n\n\nAD3\n\n\n\n\n\nAD4\n\n\nOther information about this data is given by:\n\n“HIV viral dynamic models model viral load trajectories during an anti-HIV treatment”\n“In an HIV viral dynamic model, the relationship between viral load and viral dynamic parameters is often nonlinear, and the viral dynamic parameters often vary substantially across patients.”\nThus, nonlinear mixed effect models\nAIDS researchers are also interested in the relationship between viral loads and CD4 counts over time\n“CD4 counts are known to be measured with substantial errors, and patients often drop out because of drug side effects or other problems.”\n\nWhat are some potential issues with this dataset?\n\ndifferent measurement times across patients\ndifferent numbers of within-individual measurements across patients\n\nlarge variation between patients\nlarge variation in the data within each patient\nsome patients dropping out of the study\nsome viral loads being censored (i.e., below the limit of detection)\nsubstantial measurement errors in the data\ncomplex long-term trajectories\ndata being missing at measurement times\n\n\nMultilevel models: Multilevel models/Hierarchical linear models/Nested data models: Statistical models for “nested clusters”. They contain parameters that vary at more than one level. An example could be a model of student performance, where the data are collected from students from multiple classes from multiple schools.\nOther model classes: Marginal models/GEE models – Mean and the correlation (covariance) structure are modeled separately. Does not require distributional assumptions (see Chapter 10 Wu (2019)) Transitional models – Within-individual correlation is modeled via Markov structures.\n\n\n1.1.4 Homework questions\n\nWrite down why clustered data are challenging to analyse?"
  },
  {
    "objectID": "mixed_models.html#analysing-clustered-data-with-mixed-models",
    "href": "mixed_models.html#analysing-clustered-data-with-mixed-models",
    "title": "1  Mixed Models",
    "section": "1.2 Analysing clustered data with mixed models",
    "text": "1.2 Analysing clustered data with mixed models\nA mixed model is a convenient modelling framework which can be used to model complex dependency structure within a data set. They are an extension of the familiar Normal MLR model, where the independent errors assumption is relaxed. In order to relax that assumption, a new concept is introduced: the random effect.\n\n1.2.1 Random effects\nIn a mixed effect model, the effects of each of the covariates can be split into two categories: fixed and random effects. Deciding on what is a fixed effect and what is a random effect can be difficult, and there is no agreed upon definition: see this post by Andrew Gelman. I will provide some guidance below, but ultimately, there is no binary rule for determining whether one should model an effect as fixed or random. Your model should reflect the assumptions that are reasonable to make about the data at hand, and answer the research questions adequately.\nWhen we model a fixed effect, we only model the average across the whole population. On the other hand, when we model a covariate as a random effect, we are modelling the average effect of that covariate as well as how that effect might vary between clusters. So, if we want a measure of how an effect varies between clusters, one would use a random effect. Random effects can also be used to implicitly introduce a dependency structure within clusters. For instance, in the Ozempic example in Section Section 1.1.2, we saw that the random effect introduced a correlation between the observations coming from the same individual. Furthermore, modelling the intercept as a random effect allows us to estimate the average “regression line” for the population taking Ozempic, as well as how that “regression line” varies from person to person. (This will be made precise later if you missed the lecture.)\n\nExample 1.5 A first example:\nA clinical trial is set up to compare a new drug with a standard drug. The drug effect is of interest in the trial. We propose a Normal MLR (or fixed-effects) model with “drug” and “gender” as the two-fixed effects factors. Each has finite levels: “drug” – “new drug” and “standard drug”; “gender” – “female”,“male”, “non-binary”. Is there a cluster variable? Should we introduce any random effects?\n\n\nExample 1.6 Clinical trial:\nIn a clinical trial, several hospitals in Canada are sampled. In each of the selected hospitals, a new treatment is compared with an existing treatment. Is there a cluster variable? Should we introduce any random effects? What definition(s) do any of these random effects fit?\n\nOne way to decide on whether an effect is a fixed or random effect is to ask if the observations for that covariate contain the complete set of levels we are interested in for a given covariate. In Example 1.6, the data can be clustered by hospital. The treatment is a fixed effect, as we have observed the complete set of levels we are interested in for it. On the other hand, we would like our analysis to generalize beyond the selected hospitals, and so we would like to assess how the regression line varies between hospitals. This rule does not work well for continuous covariates, such as height, which may not require a random effect, but also, we cannot observe all levels of this factor.\n\nExample 1.7 Antibiotics:\nThe efficacy an antibiotic maintains after it has been stored for two years is of scientific interest. Eight batches of the drug are selected at random from a population of available batches. From each batch, we take a sample of size two. The goal of the analysis: Estimate the overall mean concentration. Does the random batch have a significant effect on the variability of the responses?\n\n\n\nbatch\nr1\nr2\n\n\n\n\n1\n40.00\n42.00\n\n\n2\n33.00\n34.00\n\n\n3\n46.00\n47.00\n\n\n4\n55.00\n52.00\n\n\n5\n63.00\n59.00\n\n\n6\n35.00\n38.00\n\n\n7\n56.00\n56.00\n\n\n8\n34.00\n29.00\n\n\n\nSince the batches are drawn randomly from a larger population, we could model the batch effect as a random effect. Obviously, the within batch observations will be correlated. The data are clustered by batch. Suppose instead that only eight batches exist in the whole world, and we are interested in knowing whether the batch number has an effect on the response. Then, the batch becomes a fixed effect.\n\n\nExample 1.8 A Tale of Two Thieves (Cabrera and McDougall (2002)):\nRecall that the client wanted us to assess the level of active ingredient in their tablets, as well as assess the variability in that can be attributed to the sampling technique.\n\n\n\n\nMETHOD\nLOCATION\nREPLICATE\nASSAY\n\n\n\n\n1\nIntm\n1\n1\n34.38\n\n\n2\nIntm\n1\n2\n34.87\n\n\n3\nIntm\n1\n3\n35.71\n\n\n4\nIntm\n2\n1\n35.31\n\n\n5\nIntm\n2\n2\n37.59\n\n\n6\nIntm\n2\n3\n38.02\n\n\n\n\n\n\nNumber\nmethdb\ndrum\ntablet\nyb\n\n\n\n\n1\nTablet\n1\n1\n35.77\n\n\n2\nTablet\n1\n2\n39.44\n\n\n3\nTablet\n1\n3\n36.43\n\n\n4\nTablet\n5\n1\n35.71\n\n\n5\nTablet\n5\n2\n37.08\n\n\n6\nTablet\n5\n3\n36.54\n\n\n\nWhat are the clusters? What might be a random effect?\n\n\n\n1.2.2 Homework questions\n\nGive an example of a study where a mixed effect model would apply, which effects are random and which are fixed?\nDescribe the potential differences between a random and fixed effect. Compare and contrast the different definitions of random effects. Which one do you prefer and why?\nHow would you determine which definition for random effects a client is using?\n\n\n\n1.2.3 Regression and general data modelling review\nBy the end of this section, we will have covered all steps involved in analyzing data using mixed models:\n\n\n\n\nflowchart LR\n  A[Exploratory analysis] --&gt; B[Model specification]\n  B --&gt; C{Estimation}\n  C --&gt; D[Inference]\n  D --&gt; E[Diagnostic plots]\n  E --&gt; B\n  E --&gt; F[Sensitivity testing]\n\n\n\n\n\n\nExample 1.9 How do you do each of these steps in a simple linear regression model?\n\nLet’s review. Suppose \\(Y_i\\) are continuous and we want to model \\(E[Y_i |X_i ]\\). A linear regression model takes \\[E[Y_i |X_i ] = X_i'\\beta.\\] We take \\(\\hat\\beta = (X'X)^{-1}X'Y\\), and call these ordinary least squares (OLS) estimators. If \\(Y_i |X_i \\sim N(X_i'\\beta,\\sigma^2),\\) then the OLS estimators are the maximum likelihood estimators.\nIf we take \\(Y_i = X_i'\\beta + \\epsilon_i\\) , where \\(X_i\\) is non-normal, then the OLS estimators minimize the MSE of any predictor: \\[\\phi(\\beta)=\\frac{1}{n}\\sum_{i=1}^n ||\\beta-E[Y_i |X_i ] ||^2\\] is minimized at \\(\\hat\\beta\\).\nIn this case, as discussed in Section Section 1.1.1, we assume that:\n\nThe conditional mean is linear (in parameters).\nAll values of \\(Y_i\\) have constant variance, denoted \\(\\sigma^2\\) (conditionally).\nThe \\(Y_i\\) are independent.\n\nThen, one can show that \\(\\hat\\beta\\) is asymptotically normal with \\(Var(\\hat\\beta)=\\sigma^2 (X'X)^{-1}\\). We then use this fact to construct confidence intervals, hypothesis tests etc. We later analyze the residuals to diagnose any problems with the fit. Overall, linear regression allows us to estimate a functional form for the conditional mean of a continuous outcome. The ordinary least-squares estimators are valid MLE-type estimators when normality is assumed, and are least-squares estimators otherwise. The asymptotic analysis is valid in large samples, regardless of distributional assumptions. We now present equivalents of these results for the mixed model.\n\n\n1.2.4 Defining the linear mixed model\nBack to analyzing clustered data. Let’s start with longitudinal data, which could be represented by the following diagram: \nIn (goal?), we see that there are time-varying and constant covariates, as well as the time-varying response. To formlize this, we can write:\n\n\\(Y_{ij}\\) response of subject \\(i\\) at \\(j\\)th time point for \\(i\\in[n]\\) and \\(j\\in[J_i]\\).\n\\(X_{ijk}\\) covariate \\(k\\) of subject \\(i\\) at \\(j\\)th time point for \\(k\\in[K]\\), \\(i\\in[n]\\) and \\(j\\in[J_i]\\).\n\\(X_{ij}\\) covariate vector for subject \\(i\\) at \\(j\\)th time point for \\(i\\in[n]\\) and \\(j\\in[J_i]\\).\n\\(t_{ij}\\) actual time for subject \\(i\\) at time point \\(j\\) for \\(i\\in[n]\\) and \\(j\\in[J_i]\\).\n\nWe can split the covariate matrix into time-varying covariates \\(Z\\) and constant covariates \\(W\\). We have that \\(X=[W|Z].\\) Each subject has \\(J_i\\) rows in \\(X\\) associated with it. Let \\(X_{i}\\) be the \\(J_i\\times p\\) submatrix corresponding to the covariates for subject \\(i\\) and let \\(Z_i\\) be the \\(J_i\\times m\\) submatrix corresponding to the time-varying covariates for subject \\(i\\).\nNow, the goal is to fit a model for \\(E[Y_{ij} |X_{ij} , t_{ij} ]\\) with interpretable parameters.\nTo account for the correlation between subjects, we model the response as a vector \\(Y_i\\), where \\[Y_{i}=X_{i}\\alpha+Z_{i}\\beta_i+\\epsilon_{i},\\] where\n\n\\(\\alpha\\): Population level effects – constant between subjects \\((p\\times 1)\\)\n\\(\\beta_i\\): Patient-level heterogeneity – varies between subjects \\((m\\times 1)\\)\n\\(\\epsilon_{ij}\\): Individual measurement variation – varies between measurements (scalars)\n\nNote that in the mixed model, we assume: \\(\\alpha\\) is a fixed vector, \\(\\beta_i\\) is randomly drawn for each individual, \\(\\epsilon_{ij}\\) are also randomly drawn.\nWe can then assume that \\(\\beta_i\\sim N(0,\\Sigma_\\beta)\\), \\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) with \\(\\epsilon_{ij} \\perp \\beta_i\\).\nNow, let’s look at some properties of the model. Conditional on the random effects \\[E[Y_i |\\beta_i,X_i ] = X_i\\alpha + Z_i\\beta_i\\] and \\[Cov(Y_i |\\beta_i,X_i  ) = Cov(\\epsilon_i|\\beta_i,X_i ) = \\sigma^2 I.\\] Derive these. If we consider the marginal distribution of \\(Y_i\\) we find: \\[E[Y_i|X_i] = X_i\\alpha \\qquad \\text{and}\\qquad Cov(Y_i|X_i) = Z_i\\Sigma_\\beta Z_i'+\\sigma^2 I\\] Derive this. Combining these results we find that, under this assumed model, \\[Y_i|X_i\\sim N(X_i\\alpha,Z_i\\Sigma_\\beta Z_i'+\\sigma^2 I ).\\]\n\n\n1.2.5 Special cases of single layer models\nWe will now cover some special cases of the above model. The most basic mixed model is the random intercept model. Let \\(\\tilde{W}_i\\) be the covariate matrix of fixed effects with the intercept column removed. The resulting model is \\[Y_{i}=\\alpha_1\\mathbb{1}_{J_i}+\\tilde{W}_{i}\\alpha+\\beta_i\\mathbb{1}_{J_i}+\\epsilon_{i},\\] where \\(\\beta_i\\sim N(0,\\sigma^2_\\beta)\\) and \\(\\epsilon_{i}\\sim N(0,\\sigma^2 I_{J_i})\\). For \\(\\ell\\neq j\\), it follows that \\[Corr(Y_{ij},Y_{i\\ell})=\\frac{\\sigma^2_\\beta}{\\sigma^2_\\beta+\\sigma^2}.\\] The variance is constant across time or clusters: \\(Cov(Y_i|X_i)=(\\sigma^2_\\beta+\\sigma^2)I\\). Derive these. Observe that in this model, all subject level regression lines are parallel. This can be used when we suspect that the correlation is constant over time, and only the mean response is thought to vary between clusters.\nIf instead, we would like the regression lines to vary in general between subjects, we can introduce slopes are random effects. This is the random intercept and slope model. Here, \\[\nY_{i }=\\alpha_0\\mathbb{1}_{J_i}+\\widetilde{W}_{i} \\alpha+\\beta_{0i}\\mathbb{1}_{J_i}+\\alpha_1t_{i}+\\beta_{1i}t_{i}+\\epsilon_{i }.\n\\] What is \\(Z_i\\) here? The within-subject correlation will be time dependent in this model automatically, in this model, we assume that \\(\\beta_i=(\\beta_{0i},\\beta_{1i})'\\sim N(0,\\Sigma_\\beta)\\), where \\[\n\\Sigma_\\beta=\\left(\\begin{array}{cc}\n    \\sigma^2_{\\beta_0} &   \\sigma_{\\beta_0,\\beta_1} \\\\\n     \\sigma_{\\beta_0,\\beta_1}   &   \\sigma^2_{\\beta_1}\n\\end{array}\\right).\n\\]\nNow, let’s understand some of the features of this model. For any \\(i\\in[n]\\), we have that \\[Cov(Y_i|X_i) = Z_i\\Sigma_\\beta Z_i'+\\sigma^2 I=(1_{J_i}\\ t_i)\\Sigma_\\beta(1_{J_i}\\ t_i)'+\\sigma^2 I.\\] We see that the variance of the response is not constant across time. Further, for \\(\\ell\\neq j\\): \\[Cov(Y_{ij},Y_{i\\ell})=\\sigma^2_{\\beta_0}+\\sigma_{\\beta_0,\\beta_1}(t_{ij}+t_{i\\ell})+\\sigma^2_{\\beta_1}t_{ij}t_{i\\ell}.\\] In this model, the correlation between subject responses at different time points is varying.\nUse the following code to explore how the correlation between time points changes as the distance between the time points grows.\n\nplot_cor=function(matt){\n\n  Zi=cbind(rep(1,10),1:10)\n  # err=s\n  cov_mat=Zi%*%matt%*%t(Zi)+diag(rep(1,10))\n  cov_mat\n  vars=sqrt(diag(cov_mat))\n  vars\n  corr_mat=apply(cov_mat,1,\"*\",1/vars)\n  corr_mat=apply(corr_mat,1,\"*\",1/vars)\n  corr_mat\n  plot(1:10,corr_mat[1,])\n}\n\n\nmatt=matrix(c(1,0.4,0.4,1),nrow=2)\nmatt=matrix(c(1,0.4,0.4,1),nrow=2)\nmatt=matrix(c(1,0,0,2),nrow=2)\nmatt=matrix(c(1,0,0,6),nrow=2)\nmatt=matrix(c(1,-0.9,-0.9,4),nrow=2)\nmatt=matrix(c(1,0,0,0.5),nrow=2)\nplot_cor(matt)\n\n\n\n\n\n\n1.2.6 Multi-level models\nSo far we have discussed single level mixed models, which do not admit a nested structure. In this way, there is a nested structure of clusters. For example, if we wanted to assess how a new way of teaching p-values affects statistical literacy, we could sample universities, then professors, then classes. Here, assuming professors teach multiple sections, we could assume that effects differ by university, by professor, and by class. We could write a mixed effect model with 3 levels as \\[\\begin{align*}\nY_{ijk}&=X_{ijk}\\alpha+Z_{i,jk}\\beta_i+Z_{ij,k}\\beta_{ij}+Z_{ijk}\\beta_{ijk}+\\epsilon_{ijk}\\\\\n&i\\in[n],\\ j\\in[J_i],\\ k\\in [m_{ij}],\\\\\n&\\beta_i\\sim N(0,\\Sigma_1),\\ \\beta_{ij}\\sim N(0,\\Sigma_2),\\ \\beta_{ijk}\\sim N(0,\\Sigma_3),\\ \\epsilon_{ijk}\\sim N(0,\\sigma^2 I).\n\\end{align*}\\] Note that the “,” tells us which columns of the covariate matrix we are concerned with: \\(Z_{i,jk}\\) denotes the covariates nested in the highest level, \\(Z_{ij,k}\\) the second highest and \\(Z_{ijk}\\) the inner-most level. For instance, with respect to the above example, \\(Z_{i,jk}\\) would be the university level covariates. (Note that some books count the number of levels by the number of sources of random variation, which is 1+ the level definition used here.) In addition, Pinheiro and Bates (2009) represents \\(\\Sigma_1\\) in form \\(\\Sigma_1^{-1}/\\sigma^2=\\Delta'\\Delta\\), where \\(\\Delta\\) is a non-unique relative precision factor. Specifications of the covariance matrices depend on the context. For more details, see Pinheiro and Bates (2009), Gelman and Hill (2006), Wu (2019).\n\n\n1.2.7 Parameter estimation and inference\nGenerally, parameters are estimated with either maximum likelihood or restricted maximum likelihood (REML). Recall that this model is parametric, we have assumed normality. Thus, we can write down the likelihood. Let \\(V_i=Cov(Y_i)=Z_i\\Sigma_\\beta Z_i'+\\sigma^2 I\\). Then, the (familiar) asymptotic result for both the MLE and the REML estimates: \\[\\widehat{\\alpha} \\dot{\\sim} N\\left(\\alpha\\ ,\\left[\\sum_{i=1}^n X_i^{\\prime} V_i^{-1} X_i\\right]^{-1}\\right),\\]\nwhere \\(\\dot{\\sim}\\) denotes asymptotically distributed as. For more details on how to derive the estimates, see Pinheiro and Bates (2009).\nRestricted maximum likelihood is used because the MLE biases the variance estimates downward. In REML, we maximize \\[\\mathcal{L}(\\Sigma_\\beta,\\sigma^2|y)=\\int \\mathcal{L}(\\alpha,\\Sigma_\\beta,\\sigma^2|y)d\\alpha .\\] This constitutes a uniform prior on \\(\\alpha\\). Note that REML estimates are not invariant under reparameterizations of the fixed effects – changing the units of the covariates \\(X_i\\) units changes the estimates. As a result, LRT are not valid for testings significance of fixed effects – the restricted likelihoods cannot be compared to determine significance.\nTesting – Fixed effects – MLE: When using the MLEs, we can use likelihood ratio tests to test significance of various parameters. The parameters for the covariances, denoted \\(\\sigma_{\\beta_k,\\beta_\\ell}\\), will have some regularity concerns. Suppose we want to test whether a subset of the parameters are 0. Let \\(k=\\)# df in alt - # df in null. Recall that Wilks’ Theorem gives \\(-2(\\ell_1(\\hat\\theta)-\\ell_0(\\hat\\theta))\\sim \\chi^2_{k}\\), which can be used to conduct the test.\nTesting – Random effects: However, this does not apply to random effects. The variance parameters lie on the boundary of the parameter space, and so Wilks’ Theorem does not apply! Instead, we can simulate the distribution of the LRT statistic under the null and use the simulated distribution to obtain our critical value. If you are using a software where this is not feasible, then you can use \\(\\frac{1}{2} \\chi^2_{\\#\\ RE\\ Null}+\\frac{1}{2} \\chi^2_{\\#\\ RE\\ ALT}.\\)\nTesting – Fixed effects – REML: REML estimates are not invariant under reparameterizations of the fixed effects – changing the units for the covariates \\(X_i\\) changes the REML estimates. As a result, LRT are not valid for testing the significance of fixed effects – the restricted likelihoods cannot be compared to determine significance. To test the fixed effects, we can use tests conditional on the variance parameters/RE parameters. In this case, we can perform either marginal \\(t\\)-tests – tests which consider adding the parameter to the model with all other covariates, or sequential \\(F\\)-tests – a test that adds the variables sequentially in the order they enter the model.\nConfidence intervals – Fixed effects – REML: Both REML and MLE give asymptotic normality of both \\(\\hat\\sigma\\) and fixed effect estimates. This can be used to obtain confidence intervals. For the parameters contained in \\(\\Sigma_\\beta\\), constructing confidence intervals can be more difficult because \\(\\Sigma_\\beta\\) must be positive definite, which restricts the parameter space. In this case, we transform the parameters so that they are unconstrained, compute the confidence interval, and transform the interval back. See Section 2.4 in “Mixed Models in S and S-plus” for more details Pinheiro and Bates (2009).\n\n\n1.2.8 Individual effects\nOne thing we may want to do is produce an estimate of \\(\\beta_i\\) for observation \\(i\\). One may notice that \\(E[\\beta_i|Y_i]=\\Sigma_\\beta Z_i' V_i^{-1} (Y_i-X_i\\alpha)\\). Wait, we either know or have estimates of all of the values on the right-hand side. BLUP: \\(\\hat\\beta_i=\\hat\\Sigma_\\beta Z_i' \\hat V_i^{-1} (Y_i-X_i\\hat\\alpha)\\). Fitted values: \\[\\hat Y_i=X_i\\hat\\alpha+Z_i\\hat\\beta_i.\\]\nLet’s analyze \\(V_i\\):\n\\[\\begin{align*}\n   V_i &= Z_i \\Sigma_\\beta Z_i' + \\sigma^2 I \\\\\n   \\implies V_i V_i^{-1} &= Z_i \\Sigma_\\beta Z_i'V_i^{-1} + \\sigma^2 I V_i^{-1} \\\\\n   \\implies I &= Z_i \\Sigma_\\beta Z_i'V_i^{-1} + \\sigma^2 V_i^{-1}.\n\\end{align*}\\]\nNow, the same logic gives that \\[I=Z_i \\hat\\Sigma_\\beta Z_i'V_i^{-1}+\\hat\\sigma^2 V_i^{-1}.\\] We have \\[\\begin{align*}\n   \\hat Y_i&=X_i\\hat\\alpha+Z_i\\hat\\beta_i\\\\\n  &= X_i\\hat\\alpha+Z_i(\\hat\\Sigma_\\beta Z_i' \\hat V_i^{-1} (Y_i-X_i\\hat\\alpha))\\\\\n   &=(I-\\hat\\Sigma_\\beta Z_i' \\hat V_i^{-1}) X_i\\hat\\alpha+Z_i\\hat\\Sigma_\\beta Z_i' \\hat V_i^{-1} Y_i\\\\\n   &=\\hat\\sigma^2 \\hat V_i^{-1} X_i\\hat\\alpha+(I-\\hat\\sigma^2\\hat V_i^{-1}) Y_i\\\\\n     &=\\hat\\sigma^2 \\hat V_i^{-1} X_i\\hat\\alpha+Z_i\\hat\\Sigma_\\beta Z_i'V_i^{-1} Y_i.\n\\end{align*}\\] Thus, \\[\\hat Y_i=\\hat\\sigma^2 \\hat V_i^{-1} X_i\\hat\\alpha+Z_i\\hat\\Sigma_\\beta Z_i'V_i^{-1} Y_i.\\] We have that:\n\n\\(\\hat\\sigma^2 I\\) within subject variation\n\\(Z_i\\hat\\Sigma_\\beta Z_i'\\) between subject variation\nHigher within subject variation – more weight to the population average\n\n\n\n1.2.9 Exploratory analysis, checking assumptions and sensitivity testing\nExploratory analysis: Prior to setting up our model, we would ideally conduct exploratory analysis. Here, we look for outliers, inconsistencies in the data, and try to ascertain the relationships between the provided variables. This will help inform the model we will choose. It is also helpful to check that the model results approximately mirror what we saw in the EDA, as a sanity check. Some tools you can use in EDA are:\n\nDescriptive statistics\nxy plots – may have to subsample\nBox plots by cluster variable\nCross-sectional plots\n\nDiagnostic plots: These are used to check the fit of the model and check the assumptions. In a mixed model, we have independence and normality, and structure assumptions. This involves using graphics we are likely familiar with, such as qqplots. We may use:\n\nChecking independence between residuals across time – acf (may not be appropriate), variogram\nWe have independence and normality, and structure assumptions\nResiduals vs. fitted values\nqqplot of residuals/random effects for normality\nObserved vs. fitted values\n\nSensitivity testing: In reality, there may be several models/frameworks with assumptions that could fit your data. For example, we may use a nonparametric method, a robust method, inclusion of different effects, use of different statistical tests. One thing you can do after performing a data analysis is to do the analysis under other models that may have been applied, and see if your results change. This helps support the conclusions made, and can reveal additional insights about your dataset. Be careful not to apply models whose assumptions are not reasonable for your data.\n\n\n1.2.10 Homework questions\n\nHow would you analyse the TLC data discussed last class? What are some statistical tests you might conduct?\nWrite down the likelihood function under the random intercept model."
  },
  {
    "objectID": "mixed_models.html#case-study-batches-of-antibiotic-and-quality-control",
    "href": "mixed_models.html#case-study-batches-of-antibiotic-and-quality-control",
    "title": "1  Mixed Models",
    "section": "1.3 Case study: Batches of antibiotic and quality control",
    "text": "1.3 Case study: Batches of antibiotic and quality control\n\nlibrary(nlme)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\nWarning: package 'Matrix' was built under R version 4.2.3\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\n\n\n1.3.1 Case information:\n\nAfter an antibiotic has been stored for two years, it is of scientific interest to know what concentration of active ingredient is.\nEight batches of the drug are selected at random from a population of available batches.\nFrom each batch, we take a sample of size two.\nThe goal of the analysis: Determine (to estimate) the overall mean concentration. A further question is whether or not the random batch has a significant effect on the variability of the responses.\n\nFrom 8 batches of antibiotics, 2 samples are drawn.\n\n\n\nBatch\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nSample 1\n40\n33\n46\n55\n63\n35\n56\n34\n\n\nSample 2\n42\n34\n47\n52\n59\n38\n56\n29\n\n\n\n\nbatch=as.matrix(read.csv('data/batch.csv'))\n\nWarning in read.table(file = file, header = header, sep = sep, quote = quote, :\nincomplete final line found by readTableHeader on 'data/batch.csv'\n\nbatch=t(batch)\nbatch=unname(batch)\nbatch=data.frame(cbind(1:8,batch))\nnames(batch)=c(\"batch\",\"r1\",\"r2\")\nbatch$r1=as.double(batch$r1)\nbatch$r2=as.double(batch$r2)\nbatch\n\n  batch r1 r2\n1     1 40 42\n2     2 33 34\n3     3 46 47\n4     4 55 52\n5     5 63 59\n6     6 35 38\n7     7 56 56\n8     8 34 29\n\n\nOverall mean: You can just take the sample mean here – the batches have an equal number of samples in each of the batches.\n\nmean(c(batch$r1,batch$r2))\n\n[1] 44.9375\n\nsummary(batch)\n\n     batch            r1              r2       \n Min.   :1.00   Min.   :33.00   Min.   :29.00  \n 1st Qu.:2.75   1st Qu.:34.75   1st Qu.:37.00  \n Median :4.50   Median :43.00   Median :44.50  \n Mean   :4.50   Mean   :45.25   Mean   :44.62  \n 3rd Qu.:6.25   3rd Qu.:55.25   3rd Qu.:53.00  \n Max.   :8.00   Max.   :63.00   Max.   :59.00  \n\n\nGraphically, the within batch variability is low relative to the between batch.\n\npar(cex.lab=2,cex.axis=2,mfrow=c(1,1))\nplot(batch$batch,batch$r1,pch=21,bg=1,cex=2)\npoints(batch$batch,batch$r2,pch=21,bg=1,cex=2)\n\n\n\ncor(batch$r1,batch$r2)\n\n[1] 0.9672002\n\n\nOkay what about a confidence intervals for the mean? What about whether or not the random batch has a significant effect on the variability of the responses? We need a model for this.\nIt appears that the within batch mean is not constant.\n\\[Y_{ij}=\\mu+\\beta_i+\\epsilon_{ij},\\] where for \\(i\\in[8]\\) and \\(j\\in[2]\\), we have\n\n\\(Y_{ij}\\): concentration\n\\(\\mu\\): overall mean\n\\(\\beta_i\\): effect of batch \\(i\\), this effect is random!\n\\(\\epsilon_{ij}\\) random error\n\nThe assumptions are\n\n\\(\\beta_i\\sim N(0,\\sigma^2_b)\\) iid\n\\(\\epsilon_{ij}\\sim N(0,\\sigma^2)\\) iid\n\\(\\beta_i\\) is independent of \\(\\epsilon_{ij}\\)\n\nUnder these assumptions \\(E(Y_{ij})=\\mu\\) and \\(Var(Y_{ij})=\\sigma^2+\\sigma_b^2.\\)\nIn addition, we see that we capture the dependence structure: One can check that\n\n\\(Cov(Y_{i1},Y_{i2})=\\sigma_b^2\\)\n\\(Cov(Y_{i1},Y_{i'1})=0\\)\n\nRecall we are interested in whether or not the random batch has a significant effect on the variability of the responses. This means we would like to estimate \\(\\sigma_b\\) and test if it is negligible.\nLet’s estimate the parameters of this model. The relevant R package for (generalised) linear mixed models in R are nlme, lme4 and lmerTest. Let’s use REML to estimate our parameters.\n\n#We need to reshape this data into long format!\nbatch_long=reshape(batch, \n                   varying=c('r1','r2'), \n                   timevar = 'replicate',\n                   idvar = 'batch',\n                   times=c(1,2),\n                   direction = \"long\",sep = \"\")\n\nhead(batch_long)\n\n    batch replicate  r\n1.1     1         1 40\n2.1     2         1 33\n3.1     3         1 46\n4.1     4         1 55\n5.1     5         1 63\n6.1     6         1 35\n\n#using other package\n# fit.lme&lt;-lme4::lmer(r ~ 1 | batch, data=batch_long)\n# summary(fit.lme)\n\n\nrownames(batch_long) &lt;- NULL\n\n#defaults to REML\nmodel_1=lme(\n  fixed= r~1,\n  random=  ~ 1 | batch, data=batch_long )\nsummary(model_1)\n\nLinear mixed-effects model fit by REML\n  Data: batch_long \n       AIC      BIC    logLik\n  101.0371 103.1613 -47.51855\n\nRandom effects:\n Formula: ~1 | batch\n        (Intercept) Residual\nStdDev:    10.95445 2.015565\n\nFixed effects:  r ~ 1 \n              Value Std.Error DF  t-value p-value\n(Intercept) 44.9375  3.905623  8 11.50585       0\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.35131912 -0.56486600  0.09135863  0.51634786  1.12937443 \n\nNumber of Observations: 16\nNumber of Groups: 8 \n\n\nOkay, between batch variance is huge. Let’s test if its non-zero anyways. Recall that for REML estimates, the asymptotic distribution for the LRT is not the same as usual. In this case, under the null hypothesis, \\(Y_{ij}\\sim N(\\mu,\\sigma^2)\\).\nTherefore, in order to construct a hypothesis test for \\(\\sigma^2_\\beta\\), we can do the following:\n\nCompute the LRT statistic from the sample, call it \\(\\hat T\\).\nSimulate many, say \\(n_{sim}\\), new samples of the same size from the model \\(Y_{ij}\\sim N(\\mu,\\sigma^2)\\).\nFor each of the \\(n_{sim}\\) samples, compute the LRT statistic: \\(\\tilde T_1,\\ldots,\\tilde T_{n_{sim}}\\).\nThe (empirical) p-value is then the proportion of \\(\\tilde T_1,\\ldots,\\tilde T_{n_{sim}}\\) larger than \\(\\hat T\\).\n\nThus,\n\n#Step 1. Computing T-hat\nfit_null&lt;-lm(r ~ 1 , data=batch_long)\n\nobserved=lmtest::lrtest(fit_null,model_1)$Chisq[2]; observed\n\nWarning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of\nclass \"lm\", updated model is of class \"lme\"\n\n\n[1] 25.40237\n\n#Step 2. \n\n#Get the fixed effects\nfe=nlme::fixed.effects(model_1); fe\n\n(Intercept) \n    44.9375 \n\n#Get the estimated variance of the RE\nsigma_batch_est= nlme::getVarCov(model_1); sigma_batch_est\n\nRandom effects variance covariance matrix\n            (Intercept)\n(Intercept)         120\n  Standard Deviations: 10.954 \n\n#Get the estimate of sigma\nsigma_est=model_1$sigma; sigma_est\n\n[1] 2.015565\n\nn=nrow(batch_long)\nn_sim=100\n\n\n#Step 2 \nsimulated=t(replicate(n_sim,rnorm(n,fe[1],sigma_est)))\n\n# n_sim x 16\ndim(simulated)\n\n[1] 100  16\n\n#Step 3\n#takes a simulated sample y and computes the LRT for Y\ncompute_lrt=function(y){\n  \n  #create a copy of the dataset\n  batch_copy=batch_long\n  \n  #replace response with new sample\n  batch_copy$r=y\n  \n  #replace response with new sampl\n  alt=lme(\n      fixed= r~1,\n      random= r ~ 1 | batch, data=batch_copy )\n  \n  null&lt;-lm(r ~ 1 , data=batch_copy)\n  \n  test=lmtest::lrtest(null,alt)$Chisq[2]\n  \n  \n  return(test)\n}\n\n#compute LRT for each simulated sample \nts=suppressWarnings(apply(simulated, 1, compute_lrt))\n\n\n\npvalue=mean(observed&lt;=ts); pvalue\n\n[1] 0\n\nhist(ts,xlim=c(min(ts),29))\nabline(v=observed)\n\n\n\nobserved\n\n[1] 25.40237\n\n\n\nplot(model_1)\n\n\n\nqqnorm(model_1, ~ residuals(.,type=\"pearson\"))\n\n\n\nplot(ranef(model_1))\n\n\n\nplot(fixef(model_1))\n\n\n\nqqnorm(model_1, ~ ranef(.))\n\n\n\nplot(model_1, r ~ fitted(.),abline=c(0,1))\n\n\n\n\n\nintervals(model_1)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n               lower    est.    upper\n(Intercept) 35.93112 44.9375 53.94388\n\n Random Effects:\n  Level: batch \n                   lower     est.    upper\nsd((Intercept)) 6.430117 10.95445 18.66216\n\n Within-group standard error:\n   lower     est.    upper \n1.234790 2.015565 3.290036 \n\n\nResults summary:\n\nQuick summary: the mean is estimated to be 45, and we saw significant variation between batches. The batch mean concentration has standard deviation 11.\nMore on the mean: The mean concentration is estimated to be 45, at least, we estimate that the mean concentration is not below 36 and does not exceed 54.\nBatch variability: The concentration varies significantly between batches. The standard deviation of the mean batch concentration is estimated to be 12, ranging from (6,19). This means we estimate that roughly 68% of the batches have a mean concentration within 11 units of the overall mean (estimated to be 45) and 95% are within 22 units of the overall mean."
  },
  {
    "objectID": "mixed_models.html#case-study-air-pollution",
    "href": "mixed_models.html#case-study-air-pollution",
    "title": "1  Mixed Models",
    "section": "1.4 Case study: Air Pollution",
    "text": "1.4 Case study: Air Pollution\n\nlibrary(nlme)\nlibrary(lme4)\n\n\n1.4.1 Case information:\n\nSix Cities Air Pollution Data – Data on lung growth along with assorted patient information.\nHow much of lung size do age and height explain?\n\nData Columns:\n\nid: Patient ID\nht: Patient height at the corresponding visit\nage: Patient age\nbaseht: Patient height at the first visit\nbaseage: Patient age at the first visit\nlogfev1: The log of FEV1 measurement (outcome based on lung function)\n\nData Info:\n\nhttps://content.sph.harvard.edu/fitzmaur/ala2e/\nApplied LDA: Garrett Fitzmaurice, Nan Laird & James Ware\nDockery, D.W., Berkey, C.S., Ware, J.H., Speizer, F.E. and Ferris, B.G. (1983). Distribution of FVC and FEV1 in children 6 to 11 years old. American Review of Respiratory Disease, 128, 405-412.\n\n\n\n1.4.2 EDA:\n\nair_pollution &lt;- read.csv(\"data/air_pollution.csv\")\n\nLet’s explore the data.\n\npar(cex.lab=2,cex.axis=2,mfrow=c(1,1))\n\nhead(air_pollution)\n\n  id   ht     age baseht baseage logfev1\n1  1 1.20  9.3415    1.2  9.3415 0.21511\n2  1 1.28 10.3929    1.2  9.3415 0.37156\n3  1 1.33 11.4524    1.2  9.3415 0.48858\n4  1 1.42 12.4600    1.2  9.3415 0.75142\n5  1 1.48 13.4182    1.2  9.3415 0.83291\n6  1 1.50 15.4743    1.2  9.3415 0.89200\n\nsummary(air_pollution)\n\n       id              ht             age             baseht     \n Min.   :  1.0   Min.   :1.110   Min.   : 6.434   Min.   :1.110  \n 1st Qu.: 69.0   1st Qu.:1.370   1st Qu.: 9.719   1st Qu.:1.220  \n Median :129.0   Median :1.540   Median :12.597   Median :1.260  \n Mean   :135.7   Mean   :1.498   Mean   :12.568   Mean   :1.276  \n 3rd Qu.:199.0   3rd Qu.:1.620   3rd Qu.:15.368   3rd Qu.:1.320  \n Max.   :300.0   Max.   :1.790   Max.   :18.691   Max.   :1.720  \n    baseage          logfev1        \n Min.   : 6.434   Min.   :-0.04082  \n 1st Qu.: 7.135   1st Qu.: 0.54812  \n Median : 7.781   Median : 0.86710  \n Mean   : 8.030   Mean   : 0.81600  \n 3rd Qu.: 8.449   3rd Qu.: 1.09861  \n Max.   :14.067   Max.   : 1.59534  \n\n#Check for missing values\ncolSums(is.na(air_pollution))\n\n     id      ht     age  baseht baseage logfev1 \n      0       0       0       0       0       0 \n\nunique(air_pollution$baseht)\n\n [1] 1.20 1.13 1.18 1.15 1.11 1.24 1.27 1.17 1.32 1.26 1.25 1.19 1.21 1.23 1.22\n[16] 1.30 1.37 1.41 1.14 1.29 1.31 1.28 1.36 1.33 1.38 1.12 1.35 1.34 1.45 1.39\n[31] 1.16 1.58 1.60 1.40 1.42 1.72 1.46 1.48 1.52 1.49 1.56 1.53 1.43 1.44 1.59\n[46] 1.57\n\n# Is it balanced?\nn=length(unique(air_pollution$id))\n# typeof(air_pollution$id)\nbarplot(table(as.factor(air_pollution$id)))\n\n\n\n\n\nplot(air_pollution[,-1])\n\n\n\nhist(air_pollution$baseage)\n\n\n\nhist(air_pollution$baseht)\n\n\n\n# hist(air_pollution$age)\n# hist(air_pollution$ht)\n\n#Hint: height has been shown to be linearly associated with logfev1 on log scale\nair_pollution$loght=log(air_pollution$ht)\nair_pollution$logbht=log(air_pollution$baseht)\nplot(air_pollution[,c('age','loght','logfev1')])\n\n\n\nplot(air_pollution[,c('age','ht','logfev1')])\n\n\n\npar(mfrow=c(1,5))\nbx=apply(air_pollution[,c('age','baseage','logbht','loght','logfev1')],2,boxplot)\n\n\n\nlattice::xyplot(logfev1~age, air_pollution, col = 'black', type = c('l', 'p'))\n\n\n\nlattice::xyplot(logfev1~age, air_pollution[ air_pollution$id %in% sample(1:n,10),], col = 'black', type = c('l', 'p'))\n\n\n\nlattice::xyplot(logfev1~age, air_pollution[ air_pollution$id %in% sample(1:n,10),], col = 'black', type = c('l', 'p'))\n\n\n\nlattice::xyplot(logfev1~log(ht), air_pollution, col = 'black', type = c('l', 'p'))\n\n\n\n\nWhat can we conclude from this exploratory analysis?\n\n300 participants\nNo missing values\nNot balanced\nStarting age ranges widely\nSomewhat linear relationships with the response, but not quite.\nLog scale might be better for height\nHeavy tail in base age, and base height.\nParallel regression line between response and age\n\n\n\n1.4.3 Specifying\n\nLet \\(i\\) index the individuals and \\(j\\) index the \\(j\\)th age recorded for a given individual.\n\n\\[Y_{ij}=X_{ij}^\\top\\alpha+Z_{ij}^\\top\\beta_i+\\epsilon_{ij},\\] where for \\(i\\in[299]\\) and \\(j\\in [J_i]\\).\nHere,\n\n\\(Z_{ij}^\\top=(1 , age )\\), \\(X_{ij}^\\top=(1 , age , loght ...)\\)\nEach observation has a random intercept and slope, which depends on their age.\n\n\n\n1.4.4 Estimation\nLet’s estimate the parameters of this model. Let’s use REML to estimate our parameters.\n\n#defaults to REML\nmodel &lt;- nlme::lme(\n    fixed = logfev1 ~ age + log(ht) + baseage + log(baseht) ,\n    random =~age|id,\n    correlation = NULL, # Defaults to sigma^2 I \n    method = 'REML',\n    data = air_pollution\n)\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: air_pollution \n        AIC       BIC   logLik\n  -4549.882 -4499.528 2283.941\n\nRandom effects:\n Formula: ~age | id\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev      Corr  \n(Intercept) 0.110485541 (Intr)\nage         0.007078381 -0.553\nResidual    0.060237881       \n\nFixed effects:  logfev1 ~ age + log(ht) + baseage + log(baseht) \n                 Value  Std.Error   DF  t-value p-value\n(Intercept) -0.2883233 0.03871675 1692 -7.44699  0.0000\nage          0.0235286 0.00139534 1692 16.86231  0.0000\nlog(ht)      2.2371984 0.04353724 1692 51.38585  0.0000\nbaseage     -0.0165088 0.00745785  296 -2.21362  0.0276\nlog(baseht)  0.2182148 0.14552087  296  1.49954  0.1348\n Correlation: \n            (Intr) age    lg(ht) baseag\nage          0.023                     \nlog(ht)     -0.077 -0.875              \nbaseage     -0.822 -0.184  0.180       \nlog(baseht)  0.370  0.239 -0.275 -0.815\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-6.45672792 -0.52534885  0.05351814  0.60114614  2.76671671 \n\nNumber of Observations: 1993\nNumber of Groups: 299 \n\n\n\n\n1.4.5 Testing\nDo we need the baseline fixed effects? Did the height and age they entered the study at effect the outcome?\n\nlibrary(nlme)\n#marginal tests:\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: air_pollution \n        AIC       BIC   logLik\n  -4549.882 -4499.528 2283.941\n\nRandom effects:\n Formula: ~age | id\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev      Corr  \n(Intercept) 0.110485541 (Intr)\nage         0.007078381 -0.553\nResidual    0.060237881       \n\nFixed effects:  logfev1 ~ age + log(ht) + baseage + log(baseht) \n                 Value  Std.Error   DF  t-value p-value\n(Intercept) -0.2883233 0.03871675 1692 -7.44699  0.0000\nage          0.0235286 0.00139534 1692 16.86231  0.0000\nlog(ht)      2.2371984 0.04353724 1692 51.38585  0.0000\nbaseage     -0.0165088 0.00745785  296 -2.21362  0.0276\nlog(baseht)  0.2182148 0.14552087  296  1.49954  0.1348\n Correlation: \n            (Intr) age    lg(ht) baseag\nage          0.023                     \nlog(ht)     -0.077 -0.875              \nbaseage     -0.822 -0.184  0.180       \nlog(baseht)  0.370  0.239 -0.275 -0.815\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-6.45672792 -0.52534885  0.05351814  0.60114614  2.76671671 \n\nNumber of Observations: 1993\nNumber of Groups: 299 \n\n#sequential tests: \nanova(model)\n\n            numDF denDF   F-value p-value\n(Intercept)     1  1692 11406.811  &lt;.0001\nage             1  1692 16605.209  &lt;.0001\nlog(ht)         1  1692  2905.336  &lt;.0001\nbaseage         1   296     2.929  0.0881\nlog(baseht)     1   296     2.249  0.1348\n\n#based on the above, lets remove the base effects\nmodel=update(model,fixed= logfev1 ~ age+log(ht))\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: air_pollution \n        AIC       BIC   logLik\n  -4559.789 -4520.618 2286.895\n\nRandom effects:\n Formula: ~age | id\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.11107952 (Intr)\nage         0.00706045 -0.552\nResidual    0.06025488       \n\nFixed effects:  logfev1 ~ age + log(ht) \n                 Value  Std.Error   DF   t-value p-value\n(Intercept) -0.3693653 0.00916684 1692 -40.29366       0\nage          0.0230800 0.00135469 1692  17.03715       0\nlog(ht)      2.2493934 0.04176179 1692  53.86247       0\n Correlation: \n        (Intr) age   \nage     -0.208       \nlog(ht) -0.190 -0.869\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-6.44851637 -0.51754969  0.05387027  0.59382767  2.78517863 \n\nNumber of Observations: 1993\nNumber of Groups: 299 \n\n\nCan the random slope be dropped?\n\nmodel_null=update(model,random=logfev1 ~ 1|id)\nsummary(model_null)\n\nLinear mixed-effects model fit by REML\n  Data: air_pollution \n        AIC       BIC   logLik\n  -4489.806 -4461.827 2249.903\n\nRandom effects:\n Formula: logfev1 ~ 1 | id\n        (Intercept)   Residual\nStdDev:  0.09608834 0.06429074\n\nFixed effects:  logfev1 ~ age + log(ht) \n                 Value  Std.Error   DF   t-value p-value\n(Intercept) -0.3645554 0.00833614 1692 -43.73192       0\nage          0.0237689 0.00126198 1692  18.83462       0\nlog(ht)      2.2162876 0.04195216 1692  52.82893       0\n Correlation: \n        (Intr) age   \nage     -0.048       \nlog(ht) -0.218 -0.928\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-6.00485051 -0.54072380  0.06768753  0.61714951  2.86626992 \n\nNumber of Observations: 1993\nNumber of Groups: 299 \n\nLRT=anova(model_null,model)$L[2]; LRT\n\n[1] 73.98299\n\nsimmed=nlme::simulate.lme(model_null,nsim=100,m2=model)\nsim_LRT=-2*(simmed$null$REML[,2]-simmed$alt$REML[,2])\n\npval=mean(sim_LRT&gt;=LRT); print(pval)\n\n[1] 0\n\nhist(sim_LRT)\nabline(v=LRT)\n\n\n\nanova(model_null,model)\n\n           Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nmodel_null     1  5 -4489.806 -4461.827 2249.903                        \nmodel          2  7 -4559.789 -4520.618 2286.894 1 vs 2 73.98299  &lt;.0001\n\n\n\n\n1.4.6 Diagnostics\n\n## ACF - Checks that errors are independent\nplot(ACF(model), alpha = 0.01, main = \"ACF plot for independent errors.\") # This looks problematic!\n\n\n\n# This may not be the best way to check the model fit though, since there are not evenly spaced errors...\n\n# Instead we can use a 'semi-Variogram'. \n# This should fluctuate randomly around 1\nvg &lt;- Variogram(model, form = ~age|id, resType = \"pearson\")\nplot(vg, sigma=1) ## Looks okay, honestly, not the best.\n\n\n\n# Residuals vs. Fitted (no patterns)\nplot(model, main = \"Plot of residuals vs. fitted.\")\n\n\n\n# QQPlot for normality of errors\nqqnorm(model, ~ residuals(., type=\"pearson\")) # Some issues... probably\n\n\n\n# Plots for the Predicted (BLUPs)\nplot(ranef(model)) \n\n\n\nqqnorm(model, ~ranef(.)) # These look okay!\n\n\n\n# Observed vs. Fitted\nplot(model, logfev1 ~ fitted(.), abline = c(0,1), main = \"Observed vs. Fitted\")\n\n\n\nplot(model, logfev1 ~ fitted(.)|id, abline = c(0,1), main = \"Observed vs. Fitted (By Subject)\")\n\n\n\n# Could also look (e.g.) by treatment, if it existed!\n\n### Intervals\nintervals(model)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                 lower        est.       upper\n(Intercept) -0.3873448 -0.36936532 -0.35138579\nage          0.0204230  0.02308004  0.02573708\nlog(ht)      2.1674832  2.24939340  2.33130360\n\n Random Effects:\n  Level: id \n                            lower        est.        upper\nsd((Intercept))       0.095315000  0.11107952  0.129451390\nsd(age)               0.005828914  0.00706045  0.008552184\ncor((Intercept),age) -0.685485568 -0.55220425 -0.383114275\n\n Within-group standard error:\n     lower       est.      upper \n0.05812314 0.06025488 0.06246480 \n\n\n\nintervals(model)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                 lower        est.       upper\n(Intercept) -0.3873448 -0.36936532 -0.35138579\nage          0.0204230  0.02308004  0.02573708\nlog(ht)      2.1674832  2.24939340  2.33130360\n\n Random Effects:\n  Level: id \n                            lower        est.        upper\nsd((Intercept))       0.095315000  0.11107952  0.129451390\nsd(age)               0.005828914  0.00706045  0.008552184\ncor((Intercept),age) -0.685485568 -0.55220425 -0.383114275\n\n Within-group standard error:\n     lower       est.      upper \n0.05812314 0.06025488 0.06246480 \n\n### Predictions\nnew_data &lt;- data.frame(id = c(1, 25, 25, 25),\n                       age = c(18, 18, 18, 18),\n                       ht = c(1.54, 1.85, 1.7, 2),\n                       baseht = c(1.2, 1.32, 1.32, 1.32),\n                       baseage = c(9.3415, 8.0274, 8.0274, 8.0274),\n                       loght=log(c(1.54, 1.85, 1.7, 2)))\n\n# level specifies whether at the population [0] or subject [1] level\npredict(model, newdata = new_data, level = c(0,1))\n\n  id predict.fixed predict.id\n1  1      1.017324  0.9922821\n2 25      1.429870  1.3972195\n3 25      1.239667  1.2070167\n4 25      1.605236  1.5725857\n\n\n\n\n1.4.7 Results summary:\n\nAge explains a significant amount of the variability of lung size - a one year increase in age is roughly equivalent to a exp(0.023) in lung size (fev1)\nHeight also explains lung size, we see that for every 10 cm, we have that lungs are exp(2.25*log(0.1)) (fev1) bigger\nPopulation average lung size is exp(-0.308090040)\nThere is some evidence that the time at which the subject entered the study was predictive of their lung size. Investigate!\nSeems like lungs stop growing at 16 - may be no need to study after that age?"
  },
  {
    "objectID": "mixed_models.html#case-study-tale-of-two-thieves-see-cabrera2002",
    "href": "mixed_models.html#case-study-tale-of-two-thieves-see-cabrera2002",
    "title": "1  Mixed Models",
    "section": "1.5 Case study: Tale of two thieves, see Cabrera and McDougall (2002)",
    "text": "1.5 Case study: Tale of two thieves, see Cabrera and McDougall (2002)\n\nlibrary(nlme)\nlibrary(lme4)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\n\n1.5.1 Case information:\n\nConcentration Data – Data on concentration of active ingredient in tablets and samples from blender\nRecall: “Our main concern is that the amount of active ingredient is consistent in the X tablets. We would like you to analyse both samples to determine how much the active ingredient differs from tablet to tablet. We also want to compare the quality of the samples retrieved by the thieves to determine Which one is better?”\n\nSuppose we receive the following documentation:\nPrescription and over-the-counter drugs contain a mixture of both active and inactive ingredients, with the dosage determined by the amount of active ingredient in each tablet. Making sure the tablets contain the correct dosage is an important problem in the drug manufacturing industry and in this case study, we consider an experiment conducted by a pharmaceutical company to investigate sampling variability and bias associated with the manufacture of a certain type of tablet.\n\n\n1.5.2 Outline of the Problem\nTablet Manufacture The tablets were manufactured by mixing the active and inactive ingredients in a “V-blender,” so-named because it looks like a large V. (See Figure 8.1.) Mixing was achieved by rotating the V-blender in the vertical direction. After the mixture was thoroughly blended, the powder was discharged from the bottom of the V-blender and compressed into tablet form.\nUniform Content: The most important requirement of this manufacturing process was that the tablets have uniform content. That is, the correct amount of active ingredient must be present in each tablet. The content uniformity of the mixture within the V-blender will need to be assessed. Thief Sampling A “thief” instrument was used to obtain samples from different locations within the V-blender. This was essentially a long pole with a closed scoop at one end, which was plunged into the powder mixture by a mechanical device. At the appropriate depth for a given location, the scoop was opened and a sample collected. Considerable force was needed to insert a thief into the powder mixture and it was of interest to compare two types of thieves.\n\nThe Unit Dose thief collects three individual unit dose samples at each location.\nThe Intermediate Dose thief collects one large sample which is itself sampled to give three unit dose samples.\n\n\n\n1.5.3 Experiment Procedure\nThe objective of this experiment was to study bias and variability differences between the two thieves and to compare the thief-sampled results with those of the tablets. The experiment was implemented as follows.\n\nBlend the mixture in the V-blender for 20 minutes.\nTie the thieves together and use them to obtain samples from six locations within the V-blender. A schematic of the V-blender and sampling locations was shown previously.\nDischarge the powder from the V-blender and compress it to form tablets. Load tablets into 30 drums.\nSelect 10 drums and sample three tablets from each of these drums.\nAssay all samples to determine the amount of active ingredient in each sample. The specified assay value is: 35 mg/100 mg.\n\nThe locations shown in the blender represented the “desired” sampling positions for the thieves. In the actual experiment, these “fixed” positions were subject to a certain amount of variability. The samples collected by the thieves can be regarded as random within each location.\nIn the Tablet experiment, the order in which the drums were filled was recorded and this information was incorporated into the random selection procedure. Specifically, one drum was randomly selected from each triple sequence: {1, 2, 3} { 4, 5, 6} . . . {28, 29, 30}. The factor DRUM could therefore be used to test for a “time” effect in the Tablet data.\nData Columns:\n\nmethod\nlocation\nreplicate\nassay/yb\ndrum\n\nData Info: see 8.1 in Cabrera and McDougall (2002)\n\nthief=read.csv('data/thief.csv')\ntablet=read.csv('data/tablet.csv')\n\n\n\n1.5.4 EDA\nLet’s explore the data\n\npar(cex.lab=2,cex.axis=2,mfrow=c(1,1))\n\nhead(tablet)\n\n  methdb drum tablet    yb\n1 Tablet    1      1 35.77\n2 Tablet    1      2 39.44\n3 Tablet    1      3 36.43\n4 Tablet    5      1 35.71\n5 Tablet    5      2 37.08\n6 Tablet    5      3 36.54\n\nsummary(tablet)\n\n    methdb               drum          tablet        yb       \n Length:30          Min.   : 1.0   Min.   :1   Min.   :33.09  \n Class :character   1st Qu.: 7.0   1st Qu.:1   1st Qu.:35.10  \n Mode  :character   Median :15.5   Median :2   Median :35.69  \n                    Mean   :14.9   Mean   :2   Mean   :35.79  \n                    3rd Qu.:22.0   3rd Qu.:3   3rd Qu.:36.52  \n                    Max.   :28.0   Max.   :3   Max.   :39.44  \n\nhead(thief)\n\n  METHOD LOCATION REPLICATE ASSAY\n1   Intm        1         1 34.38\n2   Intm        1         2 34.87\n3   Intm        1         3 35.71\n4   Intm        2         1 35.31\n5   Intm        2         2 37.59\n6   Intm        2         3 38.02\n\nsummary(thief)\n\n    METHOD             LOCATION     REPLICATE     ASSAY      \n Length:36          Min.   :1.0   Min.   :1   Min.   :32.77  \n Class :character   1st Qu.:2.0   1st Qu.:1   1st Qu.:35.39  \n Mode  :character   Median :3.5   Median :2   Median :36.67  \n                    Mean   :3.5   Mean   :2   Mean   :36.65  \n                    3rd Qu.:5.0   3rd Qu.:3   3rd Qu.:37.88  \n                    Max.   :6.0   Max.   :3   Max.   :39.80  \n\n#Check for missing values\ncolSums(is.na(thief))\n\n   METHOD  LOCATION REPLICATE     ASSAY \n        0         0         0         0 \n\ncolSums(is.na(tablet))\n\nmethdb   drum tablet     yb \n     0      0      0      0 \n\nunique(tablet$methdb)\n\n[1] \"Tablet\"\n\n\n\ntablet$drum=as.factor(tablet$drum); tablet$drum\n\n [1] 1  1  1  5  5  5  7  7  7  11 11 11 14 14 14 17 17 17 19 19 19 22 22 22 25\n[26] 25 25 28 28 28\nLevels: 1 5 7 11 14 17 19 22 25 28\n\ne &lt;- ggplot2::ggplot(tablet, ggplot2::aes(x = drum, y=yb)) + ggplot2::geom_boxplot()\ne\n\n\n\ne &lt;- ggplot2::ggplot(tablet, ggplot2::aes(x = drum, y=yb)) + ggplot2::geom_point()\ne\n\n\n\nboxplot(ASSAY ~ LOCATION, col=as.numeric(thief$METHOD),data=thief)\n\nWarning in boxplot.default(split(mf[[response]], mf[-response], drop = drop, :\nNAs introduced by coercion\n\n\n\n\nboxplot(ASSAY ~ METHOD,data=thief)\n\n\n\ncolor=as.integer(as.factor(thief$METHOD))+1\nplot(thief$LOCATION ,thief$ASSAY,col=color,pch=22,bg=color)\n\n\n\n\nWhat can we conclude from this exploratory analysis?\n\n\n1.5.5 Specification\nLet’s tackle the first question: how much does the active ingredient in the tablets vary? Write down the model fit below.\n\nnames(tablet)[4]=\"con\"\nmodel=lme(\n  fixed= con ~1,\n  random= con ~ 1 | drum, data=tablet )\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: tablet \n      AIC      BIC  logLik\n  108.092 112.1939 -51.046\n\nRandom effects:\n Formula: con ~ 1 | drum\n        (Intercept) Residual\nStdDev:   0.6673375 1.197821\n\nFixed effects:  con ~ 1 \n             Value Std.Error DF  t-value p-value\n(Intercept) 35.789 0.3039075 20 117.7628       0\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.58945875 -0.62071916 -0.08544433  0.37232202  2.47467411 \n\nNumber of Observations: 30\nNumber of Groups: 10 \n\n\nOkay, the “between drum standard deviation” is half the residual standard deviation. Let’s test if its non-zero. Recall that for REML estimates, the asymptotic distribution for the LRT is not the same as usual. In this case, under the null hypothesis, \\(Y_{ij}\\sim N(\\mu,\\sigma^2)\\). Thus,\n\nfit_null&lt;-lm(con ~ 1 , data=tablet)\nobserved=lmtest::lrtest(fit_null,model)$Chisq[2]\n\nWarning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of\nclass \"lm\", updated model is of class \"lme\"\n\nfe=nlme::fixed.effects(model); fe\n\n(Intercept) \n     35.789 \n\nsigma_drum_est= nlme::getVarCov(model); sigma_drum_est\n\nRandom effects variance covariance matrix\n            (Intercept)\n(Intercept)     0.44534\n  Standard Deviations: 0.66734 \n\nsigma_est=model$sigma; sigma_est\n\n[1] 1.197821\n\nn=nrow(tablet)\nn_sim=100\n\nsimulated=replicate(n,rnorm(n_sim,fe[1],sigma_est))\n\n# n_sim x n\n# dim(simulated)\n\ncompute_lrt=function(y){\n  tablet_copy=tablet\n  tablet_copy$con=y\n  \n  alt=lme(\n      fixed= con~1,\n      random= con ~ 1 | drum, data=tablet_copy )\n  \n  null&lt;-lm(con ~ 1 , data=tablet_copy)\n  \n  test=lmtest::lrtest(null,alt)$Chisq[2]\n  return(test)\n}\n\nts=suppressWarnings(apply(simulated, 1, compute_lrt))\n\n\n# ts\n\npvalue=mean(observed&lt;=ts); pvalue\n\n[1] 0.95\n\nhist(ts)\nabline(v=observed); observed\n\n\n\n\n[1] 0.4731465\n\n1-pchisq(observed,1)/2-(observed&gt;0)/2\n\n[1] 0.2457716\n\nlmtest::lrtest(fit_null,model)\n\nWarning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of\nclass \"lm\", updated model is of class \"lme\"\n\n\nLikelihood ratio test\n\nModel 1: con ~ 1\nModel 2: con ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1   2 -51.283                     \n2   3 -51.046  1 0.4731     0.4915\n\n\n\nhist(tablet$con)\n\n\n\n\n\n\n1.5.6 Diagnostics\n\nplot(model)\n\n\n\nqqnorm(model, ~ residuals(.,type=\"pearson\"))\n\n\n\nplot(ranef(model))\n\n\n\nqqnorm(model, ~ ranef(.))\n\n\n\nplot(model, con ~ fitted(.),abline=c(0,1))\n\n\n\nfit_null&lt;-lm(con ~ 1 , data=tablet)\n\n\nconfint(fit_null)\n\n               2.5 %   97.5 %\n(Intercept) 35.28119 36.29681\n\n\nLet’s tackle the next question: which sampling method is better? – which sampling method has a lower variability?\n\nlibrary(ggplot2)\npar(cex.lab=2,cex.axis=2,mfrow=c(1,1))\nboxplot(ASSAY~METHOD,data=thief)\n\n\n\nthief$LOCATION=as.factor(thief$LOCATION)\ne &lt;- ggplot(thief, aes(x = LOCATION, y=ASSAY,fill=METHOD)) + geom_boxplot()\ne\n\n\n\nthief$LOCATION=as.factor(thief$LOCATION)\nthief$METHOD=as.factor(thief$METHOD)\ne &lt;- ggplot(thief, aes(x = LOCATION, y=ASSAY, color=METHOD)) + geom_point()\ne\n\n\n\n# boxplot(ASSAY ~ LOCATION, col=as.numeric(thief$METHOD),data=thief)\nthief_wide=reshape(thief[,1:4],timevar='REPLICATE', idvar=c('LOCATION','METHOD'),  direction = \"wide\",v.names='ASSAY')\nhead(thief_wide)\n\n   METHOD LOCATION ASSAY.1 ASSAY.2 ASSAY.3\n1    Intm        1   34.38   34.87   35.71\n4    Intm        2   35.31   37.59   38.02\n7    Intm        3   36.71   36.56   35.92\n10   Intm        4   37.80   37.41   38.00\n13   Intm        5   36.28   36.63   36.62\n16   Intm        6   38.89   39.80   37.84\n\ncorrplot::corrplot(cor(thief_wide[,3:5]))\n\n\n\ncorrplot::corrplot(cor(thief_wide[1:6,3:5]),main=\"Intm\")\n\n\n\ncorrplot::corrplot(cor(thief_wide[7:12,3:5]),main=\"Unit\")\n\n\n\n\n\nWe have 6 observations per location, three for each sampling type\nEach location could have its own mean concentration – see EDA \\(\\mu+\\beta_i\\)\nWe expect that the assays within each sampling type will vary around their location means - \\(\\beta_i\\)\nNested within the locations is the replicates , 3 per sampling method\nWe can capture the variability of sampling at each location via the random effect location.\nLet \\(i\\) be the location number, \\(j\\) be the sampling type and \\(k\\) be the replicate number\n\n\\[Y_{ijk}=\\mu+\\beta_i+\\alpha_j+\\epsilon_{ijk}.\\]\nWe can also write this as follows:\n\nLet \\(i\\) be the location number and \\(j\\) be the sampling type\nLet \\(W_{ij}=(1,j-1)\\)\nLet \\(Z_{ij}=1\\) with \\(\\beta_i\\sim N(0, \\Sigma_\\beta)\\).\n\n\\[Y_{ij}=W_{ij}\\alpha+Z_{ij}\\beta_{i}+\\epsilon_{ij}.\\]\n\nthief=thief[,-5]\nthief$REPLICATE=as.factor(thief$REPLICATE)\nthief$LOCATION=as.factor(thief$LOCATION)\nthief$METHOD=as.factor(thief$METHOD)\nprint(thief$METHOD)\n\n [1] Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm Intm\n[16] Intm Intm Intm Unit Unit Unit Unit Unit Unit Unit Unit Unit Unit Unit Unit\n[31] Unit Unit Unit Unit Unit Unit\nLevels: Intm Unit\n\n#Unit is True\nthief$ASSAY=as.numeric(thief$ASSAY)\nthief\n\n   METHOD LOCATION REPLICATE ASSAY\n1    Intm        1         1 34.38\n2    Intm        1         2 34.87\n3    Intm        1         3 35.71\n4    Intm        2         1 35.31\n5    Intm        2         2 37.59\n6    Intm        2         3 38.02\n7    Intm        3         1 36.71\n8    Intm        3         2 36.56\n9    Intm        3         3 35.92\n10   Intm        4         1 37.80\n11   Intm        4         2 37.41\n12   Intm        4         3 38.00\n13   Intm        5         1 36.28\n14   Intm        5         2 36.63\n15   Intm        5         3 36.62\n16   Intm        6         1 38.89\n17   Intm        6         2 39.80\n18   Intm        6         3 37.84\n19   Unit        1         1 33.94\n20   Unit        1         2 34.72\n21   Unit        1         3 34.10\n22   Unit        2         1 39.11\n23   Unit        2         2 37.51\n24   Unit        2         3 37.79\n25   Unit        3         1 37.46\n26   Unit        3         2 34.12\n27   Unit        3         3 35.94\n28   Unit        4         1 38.05\n29   Unit        4         2 34.82\n30   Unit        4         3 35.42\n31   Unit        5         1 36.52\n32   Unit        5         2 38.60\n33   Unit        5         3 38.16\n34   Unit        6         1 39.16\n35   Unit        6         2 32.77\n36   Unit        6         3 36.95\n\n\n\nmodel=lme(\n  fixed= ASSAY ~ METHOD ,\n  random=  ~ 1|LOCATION, data=thief)\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: thief \n       AIC      BIC   logLik\n  142.2532 148.3586 -67.1266\n\nRandom effects:\n Formula: ~1 | LOCATION\n        (Intercept) Residual\nStdDev:   0.9596085 1.456579\n\nFixed effects:  ASSAY ~ METHOD \n               Value Std.Error DF  t-value p-value\n(Intercept) 36.90778 0.5209056 29 70.85310  0.0000\nMETHODUnit  -0.51111 0.4855263 29 -1.05269  0.3012\n Correlation: \n           (Intr)\nMETHODUnit -0.466\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.94429462 -0.46995636  0.02331003  0.53623214  1.53118448 \n\nNumber of Observations: 36\nNumber of Groups: 6 \n\nprint(ranef(model))\n\n  (Intercept)\n1  -1.4683710\n2   0.6522971\n3  -0.3857585\n4   0.1910729\n5   0.3488284\n6   0.6619311\n\n\n\n#Step 1. Computing T-hat\nfit_null&lt;-lm(ASSAY ~ 1 , data=thief)\n\nobserved=lmtest::lrtest(fit_null,model)$Chisq[2]; observed\n\nWarning in modelUpdate(objects[[i - 1]], objects[[i]]): original model was of\nclass \"lm\", updated model is of class \"lme\"\n\n\n[1] 5.442094\n\n#Step 2. \n\n#Get the fixed effects\nfe=nlme::fixed.effects(model); fe\n\n(Intercept)  METHODUnit \n 36.9077778  -0.5111111 \n\n#Get the estimated variance of the RE\nsigma_b_est= nlme::getVarCov(model); sigma_b_est\n\nRandom effects variance covariance matrix\n            (Intercept)\n(Intercept)     0.92085\n  Standard Deviations: 0.95961 \n\n#Get the estimate of sigma\nsigma_est=model$sigma; sigma_est\n\n[1] 1.456579\n\nn=nrow(thief)\nn_sim=100\n\n\n#Step 2 \nsimulated=t(replicate(n_sim,rnorm(n,fe[1],sigma_est))); dim(simulated)\n\n[1] 100  36\n\n#Step 3\n#takes a simulated sample y and computes the LRT for Y\ncompute_lrt=function(y){\n  \n  #create a copy of the dataset\n  t_copy=thief\n  \n  #replace response with new sample\n  t_copy$ASSAY=y\n  \n  #replace response with new sampl\n  alt=lme(\n      fixed= ASSAY~1,\n      random=ASSAY ~ 1 | LOCATION, data=t_copy )\n  \n  null&lt;-lm(ASSAY ~ 1 , data=t_copy)\n  \n  test=lmtest::lrtest(null,alt)$Chisq[2]\n  \n  \n  return(test)\n}\n\n#compute LRT for each simulated sample \nts=suppressWarnings(apply(simulated, 1, compute_lrt))\n\n\n\npvalue=mean(observed&lt;=ts); pvalue\n\n[1] 0\n\nhist(ts,xlim=c(min(ts),9))\nabline(v=observed)\n\n\n\nobserved\n\n[1] 5.442094\n\n\nSo then, the location effect introduces significant variability. We should recommend that they continue to sample multiple locations. The unit dose thief seems to estimate lower values of concentration, though this is erased by the standard error. Now, in general, we can see that this method is more variable, and potentially tends to underestimate the active ingredient. This fact leads me to recommend the intermediate sampling method.\n\nAre the assay values generally well behaved?\nIs there any evidence of a location effect\nAre the thief-sampled values comparable to the tablet values?\nDo the tablet data show any drum or time effect?\n\n\nmodel=lme(\n      fixed= ASSAY~1,\n      random= ASSAY ~ 1 | LOCATION , data=thief)\n\n\nplot(model)\n\n\n\nqqnorm(model, ~ residuals(.,type=\"pearson\"))\n\n\n\nplot(ranef(model))\n\n\n\nqqnorm(ranef(model)[,1])\n\n\n\nplot(model, ASSAY ~ fitted(.),abline=c(0,1))\n\n\n\n# fit_null&lt;-lm(ASSAY ~ 1 , data=thief)\n# confint(fit_null)\n# fit_null\n\nWe can also fit a random effect for each of the methods and locations:\n\nmodel=lme(\n     fixed= ASSAY~METHOD,\n     random= ASSAY ~ METHOD | LOCATION , data=thief)\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: thief \n       AIC      BIC    logLik\n  145.3404 154.4986 -66.67022\n\nRandom effects:\n Formula: ASSAY ~ METHOD | LOCATION\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev   Corr  \n(Intercept) 1.045122 (Intr)\nMETHODUnit  1.021304 -0.363\nResidual    1.360833       \n\nFixed effects:  ASSAY ~ METHOD \n               Value Std.Error DF  t-value p-value\n(Intercept) 36.90778 0.5337867 29 69.14331  0.0000\nMETHODUnit  -0.51111 0.6161222 29 -0.82956  0.4136\n Correlation: \n           (Intr)\nMETHODUnit -0.509\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.8314642 -0.4546280  0.0113704  0.5126063  1.8641882 \n\nNumber of Observations: 36\nNumber of Groups: 6 \n\n# We can see that the variability coming from each source is somewhat large. \n\n\n\nplot(ranef(model))\n\n\n\nplot(model)\n\n\n\nqqnorm(model, ~residuals(., type = \"normalized\"))\n\n\n\nboxplot(residuals(model, type = \"normalized\"))\n\n\n\n\n\nlarge=which.max(abs(residuals(model, type = \"normalized\"))); large\n\n 6 \n35 \n\nmodel=lme(\n     fixed= ASSAY~METHOD,\n     random= ASSAY ~ METHOD | LOCATION , data=thief[-large,])\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: thief[-large, ] \n       AIC      BIC   logLik\n  128.7654 137.7445 -58.3827\n\nRandom effects:\n Formula: ASSAY ~ METHOD | LOCATION\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr  \n(Intercept) 1.1562644 (Intr)\nMETHODUnit  0.7965863 0.066 \nResidual    1.0572951       \n\nFixed effects:  ASSAY ~ METHOD \n               Value Std.Error DF  t-value p-value\n(Intercept) 36.90778 0.5337871 28 69.14326  0.0000\nMETHODUnit  -0.21293 0.4843533 28 -0.43961  0.6636\n Correlation: \n           (Intr)\nMETHODUnit -0.201\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-1.8620371 -0.5179546  0.0419608  0.6032329  1.5075857 \n\nNumber of Observations: 35\nNumber of Groups: 6 \n\nplot(ranef(model))\n\n\n\nplot(model)\n\n\n\nqqnorm(model, ~residuals(., type = \"normalized\"))\n\n\n\nboxplot(residuals(model, type = \"normalized\"))\n\n\n\nlarge=which.max(residuals(model, type = \"normalized\"))\n\n\nmodel=lme(\n     fixed= sqrt(ASSAY)~METHOD,\n     random= sqrt(ASSAY) ~ METHOD | LOCATION , data=thief[-large,])\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: thief[-large, ] \n        AIC       BIC   logLik\n  -23.94854 -14.96949 17.97427\n\nRandom effects:\n Formula: ASSAY ~ METHOD | LOCATION\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.08767712 (Intr)\nMETHODUnit  0.10625944 -0.405\nResidual    0.10789056       \n\nFixed effects:  sqrt(ASSAY) ~ METHOD \n                Value  Std.Error DF   t-value p-value\n(Intercept)  6.074146 0.04390786 28 138.33847  0.0000\nMETHODUnit  -0.054387 0.05677613 28  -0.95792  0.3463\n Correlation: \n           (Intr)\nMETHODUnit -0.511\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.87783191 -0.44462638  0.03546945  0.45330842  2.06501422 \n\nNumber of Observations: 35\nNumber of Groups: 6 \n\nplot(ranef(model))\n\n\n\nplot(model)\n\n\n\nqqnorm(model, ~residuals(., type = \"normalized\"))\n\n\n\nboxplot(residuals(model, type = \"normalized\"))\n\n\n\nlarge=which.max(residuals(model, type = \"normalized\"))\n\n\nthief$CEN=abs(thief$ASSAY-mean(thief$ASSAY))\nmodel=lme(\n  fixed= CEN ~ METHOD ,\n  random=  ~ 1|LOCATION, data=thief)\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: thief \n       AIC      BIC    logLik\n  99.44747 105.5529 -45.72373\n\nRandom effects:\n Formula: ~1 | LOCATION\n        (Intercept)  Residual\nStdDev:   0.5372663 0.7715252\n\nFixed effects:  CEN ~ METHOD \n                Value Std.Error DF  t-value p-value\n(Intercept) 1.0988889 0.2849187 29 3.856850  0.0006\nMETHODUnit  0.5922222 0.2571751 29 2.302798  0.0287\n Correlation: \n           (Intr)\nMETHODUnit -0.451\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.59227946 -0.67605167 -0.04425782  0.47231782  2.05364082 \n\nNumber of Observations: 36\nNumber of Groups: 6 \n\nprint(ranef(model))\n\n  (Intercept)\n1  0.47423228\n2 -0.03335199\n3 -0.42613374\n4 -0.08117489\n5 -0.54024717\n6  0.60667552\n\nhist(thief$CEN)\n\n\n\n# Non-normal (of course... maybe use simulation)\n\nResults summary:\n\nTablet mean concentration was estimated to be 35.8 with CI (35.3, 36.3)\nActive ingredient was uniform across blender and sampling types\nTablets in the drums on the end points seem to have a higher active ingredient\nThe blender has higher concentrations of active ingredient 36.65 with CI (36.0743, 37.23015)\nThe location has significant variance (~1 mg)\nSampling methods are equivalent in mean, but UNIT seems to have higher variance. Possibly produce smaller estimates of the concentration, but this is not statistically significant"
  },
  {
    "objectID": "mixed_models.html#case-study-treatment-of-lead-exposed-children",
    "href": "mixed_models.html#case-study-treatment-of-lead-exposed-children",
    "title": "1  Mixed Models",
    "section": "1.6 Case study: Treatment of Lead-Exposed Children",
    "text": "1.6 Case study: Treatment of Lead-Exposed Children\nDoes treatment \\(A\\) (chelation treatment with succimer) affect the levels of lead in the blood of lead-exposed children?\nData source or Data source\nDescription:\nThe Treatment of Lead-Exposed Children (TLC) trial was a placebo-controlled,randomized study of succimer (a chelating agent) in children with blood lead levels of 20-44 micrograms/dL. These data consist of four repeated measurements of blood lead levels obtained at baseline (or week 0), week 1, week 4, and week 6 on 100 children who were randomly assigned to chelation treatment with succimer or placebo.\nData Column Descriptions:\n\nID: Subject ID Number\nTreatment: Which treatment group (P=Placebo; A=Succimenr)\nW0, W1, W4, W6: Blood-lead levels in micrograms per deciliter at Weeks 0, 1, 4, and 6\n\n\n################################################################################\nTLC &lt;- read.csv(\"data/TLC.csv\",stringsAsFactors = T)\nhead(TLC)\n\n  ID Treatment   W0   W1   W4   W6\n1  1         P 30.8 26.9 25.8 23.8\n2  2         A 26.5 14.8 19.5 21.0\n3  3         A 25.8 23.0 19.1 23.2\n4  4         P 24.7 24.5 22.0 22.5\n5  5         A 20.4  2.8  3.2  9.4\n6  6         A 20.4  5.4  4.5 11.9\n\ndim(TLC)\n\n[1] 100   6\n\nTLC$ID=as.factor(TLC$ID)\nTLC$ID=as.factor(TLC$ID)\n\n\n1.6.1 Modelling:\n\nRecall that the goal is to answer: “Does treatment \\(A\\) affect the levels of lead in the blood of lead-exposed children?”\nHow are treatment group and time related to lead levels?\nGiven treatment and time, what do we expect the blood levels to be \\(E(W|Treatment,Time)\\)?\n\nPutting this data into our notation gives:\n\n\\(n=100\\), \\(J=4\\), \\(K=1\\)\n\\(Y_{ij}\\) is the lead level of individual \\(i\\) at time \\(j\\)\n\\(X_{ij}\\) is treatment indicator of individual \\(i\\) at time \\(j\\)\n\\(t_j\\in\\{0,1,4,6\\}\\)\n\nLet’s explore this data a bit. Notice that the data is in “wide format”. To convert to a between long and wide format use reshape()\n\nhead(TLC)\n\n  ID Treatment   W0   W1   W4   W6\n1  1         P 30.8 26.9 25.8 23.8\n2  2         A 26.5 14.8 19.5 21.0\n3  3         A 25.8 23.0 19.1 23.2\n4  4         P 24.7 24.5 22.0 22.5\n5  5         A 20.4  2.8  3.2  9.4\n6  6         A 20.4  5.4  4.5 11.9\n\n# Convert from \"wide\" to \"long\" and back again, using reshape.\n# If you're interested, you can also use `pivot_wider` and `pivot_longer` from the tidyverse\n#   (If that doesn't mean anything to you, feel free to ignore it!)\nTLC_long &lt;- reshape(data = TLC,\n                    varying = c(\"W0\", \"W1\", \"W4\", \"W6\"),\n                    timevar = \"week\",\n                    idvar = \"ID\",\n                    times = c(0, 1, 4, 6),\n                    direction = \"long\",\n                    sep = \"\")\nhead(TLC_long)\n\n    ID Treatment week    W\n1.0  1         P    0 30.8\n2.0  2         A    0 26.5\n3.0  3         A    0 25.8\n4.0  4         P    0 24.7\n5.0  5         A    0 20.4\n6.0  6         A    0 20.4\n\n\n\nTLC_wide &lt;- reshape(data = TLC_long,\n                    timevar = \"week\",\n                    v.names = \"W\",\n                    idvar = \"ID\",\n                    times = c(0, 1, 4, 6),\n                    direction = \"wide\",\n                    sep = \"\")\n\nhead(TLC_wide)\n\n    ID Treatment   W0   W1   W4   W6\n1.0  1         P 30.8 26.9 25.8 23.8\n2.0  2         A 26.5 14.8 19.5 21.0\n3.0  3         A 25.8 23.0 19.1 23.2\n4.0  4         P 24.7 24.5 22.0 22.5\n5.0  5         A 20.4  2.8  3.2  9.4\n6.0  6         A 20.4  5.4  4.5 11.9\n\n\n\n#balanced\ntable(TLC_long$ID)\n\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4 \n\n#Check for missing values\ncolSums(is.na(TLC_long))\n\n       ID Treatment      week         W \n        0         0         0         0 \n\n# Create a Basic Boxplot to get a Sense of the Data\nboxplot(W ~ week + Treatment, data = TLC_long)\nabline(v=4.5) # Abline v=... draws a vertical line at 4.5 \n\n\n\n\n\n# Start with an xyplot \n# This requires the package 'lattice'\n# You can install using: install.packages(\"lattice\")\n\nlattice::xyplot(W ~ week | Treatment, \n                data = TLC_long, \n                groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\n\n\n# The plot is a mess, as-is, so instead we can subset!\nplot_num &lt;- 5 # Select a fixed number\n\n# This is Just Randomly Sampling from Each Group\nrandom_samples_P &lt;- sample(unique(TLC_long$ID[which(TLC_long$Treatment == 'P')]), \n                         size = plot_num,\n                         replace = FALSE)\nrandom_samples_A &lt;- sample(unique(TLC_long$ID[which(TLC_long$Treatment == 'A')]), \n                           size = plot_num,\n                           replace = FALSE)\n\n\n\n\n\n## Actually Draw the Plots\nplot(W ~ week, data = TLC_long, subset = (Treatment == 'A'))\nfor (rid in random_samples_A){\n  lines(W ~ week, \n        data = TLC_long, \n        subset = (ID==rid), \n        type = 'l')\n}\n\n\n\n# Repeat it for Placebo \n\npar(mfrow=c(1,2))\nplot(W ~ week, data = TLC_long, subset = (Treatment == 'P'))\nfor (rid in random_samples_P){\n  # Loop through the Random Points and Draw the Corresponding Lines\n  lines(W ~ week, \n        data = TLC_long, \n        subset = (ID==rid), \n        type = 'l')\n}\n\n\n\n\nCorrelation plot\n\n# This is a basic correlation plot\n# It requires the 'corrplot' library, which can be installed with\n# install.packages(\"corrplot\")\ncorrplot::corrplot.mixed(cor(TLC_wide[c(\"W0\",\"W1\",\"W4\",\"W6\")]), \n                         lower = 'number',\n                         upper = 'square')\n\n\n\n\nWhat can we conclude from the EDA? What model could we propose in this case?\n\n\n1.6.2 Longitudinal Data as a mixed effects model\n\nLooking at the XY plots, we see that the individual means seem to vary.\nThis means that each individual is likely to have a different mean\nThe treatment effect should be modelled as a fixed effect\nHow to model time effect?\n\n\nlattice::xyplot(W ~ week | Treatment, \n                data = TLC_long, \n                groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\n\nWhat are some ways we can model the time effect here?\nLet’s fit a linear mixed effects model.\n\n# head(TLC_long\n# head(TLC_long[order(TLC_long$ID),])\ntypeof(TLC_long)\n\n[1] \"list\"\n\n\n\n#REML\n#treat the time points as factors\nTLC_long$week=as.factor(TLC_long$week)\n# head(TLC_long)\nmodel &lt;- nlme::lme(fixed= W ~ 1 +Treatment+week+Treatment*week,\n                   random= ~1|ID, data = TLC_long) #to run the model\n\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long \n       AIC      BIC    logLik\n  2480.621 2520.334 -1230.311\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.112717 4.214287\n\nFixed effects:  W ~ 1 + Treatment + week + Treatment * week \n                   Value Std.Error  DF    t-value p-value\n(Intercept)       26.540 0.9370175 294  28.323912  0.0000\nTreatmentP        -0.268 1.3251428  98  -0.202242  0.8401\nweek1            -13.018 0.8428574 294 -15.445080  0.0000\nweek4            -11.026 0.8428574 294 -13.081691  0.0000\nweek6             -5.778 0.8428574 294  -6.855252  0.0000\nTreatmentP:week1  11.406 1.1919804 294   9.568950  0.0000\nTreatmentP:week4   8.824 1.1919804 294   7.402807  0.0000\nTreatmentP:week6   3.152 1.1919804 294   2.644339  0.0086\n Correlation: \n                 (Intr) TrtmnP week1  week4  week6  TrtP:1 TrtP:4\nTreatmentP       -0.707                                          \nweek1            -0.450  0.318                                   \nweek4            -0.450  0.318  0.500                            \nweek6            -0.450  0.318  0.500  0.500                     \nTreatmentP:week1  0.318 -0.450 -0.707 -0.354 -0.354              \nTreatmentP:week4  0.318 -0.450 -0.354 -0.707 -0.354  0.500       \nTreatmentP:week6  0.318 -0.450 -0.354 -0.354 -0.707  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.18502705 -0.46501691 -0.04732964  0.36499878  7.66712566 \n\nNumber of Observations: 400\nNumber of Groups: 100 \n\nnlme::intervals(model)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                       lower    est.      upper\n(Intercept)       24.6958881  26.540  28.384112\nTreatmentP        -2.8977028  -0.268   2.361703\nweek1            -14.6767987 -13.018 -11.359201\nweek4            -12.6847987 -11.026  -9.367201\nweek6             -7.4367987  -5.778  -4.119201\nTreatmentP:week1   9.0601043  11.406  13.751896\nTreatmentP:week4   6.4781043   8.824  11.169896\nTreatmentP:week6   0.8061043   3.152   5.497896\n\n Random Effects:\n  Level: ID \n                  lower     est.    upper\nsd((Intercept)) 4.33785 5.112717 6.025997\n\n Within-group standard error:\n   lower     est.    upper \n3.887059 4.214287 4.569062 \n\n\nWhat do we notice here?\n\nWe see that the treatment effect is not significant in this model, but the interaction terms are.\n\n\n#we can plot the xy plot of the fitted values\n\n\nyhat=predict(model,newdata = TLC_long[,-4],level=0:1)\nTLC_long_2=TLC_long\nTLC_long_2$yhat=yhat[,3]\nTLC_long_2$week2=as.numeric(as.character(TLC_long_2$week))\n\nlattice::xyplot(yhat ~ week2|Treatment,data=TLC_long_2  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'),ylim=c(0,60))\n\n\n\n# residuals over time?\n\n# Residuals vs. Fitted (no patterns)\nplot(model, main = \"Plot of residuals vs. fitted.\")\n\n\n\n# QQPlot for normality of errors\nqqnorm(model, ~ residuals(., type=\"pearson\")) # Some issues... probably\n\n\n\n# Plots for the Predicted (BLUPs)\nplot(nlme::ranef(model)) \n\n\n\nqqnorm(model, ~ranef(.)) # These look okay!\n\n\n\n# model$residuals\n\n\n# Observed vs. Fitted\nplot(model, W ~ fitted(.), abline = c(0,1), main = \"Observed vs. Fitted\")\n\n\n\nplot(model, W~ fitted(.)|ID, abline = c(0,1), main = \"Observed vs. Fitted (By Subject)\")\n\n\n\n# Could also look (e.g.) by treatment, if it existed!\n\nSome thoughts\n\nMaybe we should investigate this residual… If this patient is outlying for indiosyncratic reasons we may want to remove them and redo the analysis\nThe fitted xy plots look like the empirical ones - good sign\nEverything else looks pretty good\n\n\nid=TLC_long$ID[which(residuals(model, type=\"pearson\")&gt;5)]\n\nplot(TLC_long[TLC_long$ID==id,-1])\n\n\n\ncol=rep(1,400)\ncol[TLC_long$ID==id]=4\n\nlwd=rep(1,400)\nlwd[TLC_long$ID==id]=4\n\nlattice::xyplot(W ~ week | Treatment, \n                data = TLC_long, \n                groups = ID, \n                col = col, \n                type = c('l', 'p'),lwd=lwd)\n\n\n\nqqnorm(model, ~ranef(.)) \n\n\n\n\n\nTLC_long_3=TLC_long[TLC_long$ID!=id,]\n\nmodel2 &lt;- nlme::lme(fixed= W ~ 1 +Treatment+week+Treatment*week,random= ~1|ID, data = TLC_long_3) #to run the model\n\nsummary(model)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long \n       AIC      BIC    logLik\n  2480.621 2520.334 -1230.311\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.112717 4.214287\n\nFixed effects:  W ~ 1 + Treatment + week + Treatment * week \n                   Value Std.Error  DF    t-value p-value\n(Intercept)       26.540 0.9370175 294  28.323912  0.0000\nTreatmentP        -0.268 1.3251428  98  -0.202242  0.8401\nweek1            -13.018 0.8428574 294 -15.445080  0.0000\nweek4            -11.026 0.8428574 294 -13.081691  0.0000\nweek6             -5.778 0.8428574 294  -6.855252  0.0000\nTreatmentP:week1  11.406 1.1919804 294   9.568950  0.0000\nTreatmentP:week4   8.824 1.1919804 294   7.402807  0.0000\nTreatmentP:week6   3.152 1.1919804 294   2.644339  0.0086\n Correlation: \n                 (Intr) TrtmnP week1  week4  week6  TrtP:1 TrtP:4\nTreatmentP       -0.707                                          \nweek1            -0.450  0.318                                   \nweek4            -0.450  0.318  0.500                            \nweek6            -0.450  0.318  0.500  0.500                     \nTreatmentP:week1  0.318 -0.450 -0.707 -0.354 -0.354              \nTreatmentP:week4  0.318 -0.450 -0.354 -0.707 -0.354  0.500       \nTreatmentP:week6  0.318 -0.450 -0.354 -0.354 -0.707  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.18502705 -0.46501691 -0.04732964  0.36499878  7.66712566 \n\nNumber of Observations: 400\nNumber of Groups: 100 \n\nsummary(model2)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_3 \n      AIC     BIC    logLik\n  2371.01 2410.62 -1175.505\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.083126 3.671238\n\nFixed effects:  W ~ 1 + Treatment + week + Treatment * week \n                      Value Std.Error  DF    t-value p-value\n(Intercept)       26.393878 0.8957514 291  29.465627  0.0000\nTreatmentP        -0.121878 1.2604340  97  -0.096695  0.9232\nweek1            -12.900000 0.7417021 291 -17.392428  0.0000\nweek4            -10.859184 0.7417021 291 -14.640897  0.0000\nweek6             -6.512245 0.7417021 291  -8.780135  0.0000\nTreatmentP:week1  11.288000 1.0436674 291  10.815707  0.0000\nTreatmentP:week4   8.657184 1.0436674 291   8.294964  0.0000\nTreatmentP:week6   3.886245 1.0436674 291   3.723643  0.0002\n Correlation: \n                 (Intr) TrtmnP week1  week4  week6  TrtP:1 TrtP:4\nTreatmentP       -0.711                                          \nweek1            -0.414  0.294                                   \nweek4            -0.414  0.294  0.500                            \nweek6            -0.414  0.294  0.500  0.500                     \nTreatmentP:week1  0.294 -0.414 -0.711 -0.355 -0.355              \nTreatmentP:week4  0.294 -0.414 -0.355 -0.711 -0.355  0.500       \nTreatmentP:week6  0.294 -0.414 -0.355 -0.355 -0.711  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.63582605 -0.49846350 -0.05679961  0.40026479  3.28119867 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\nnlme::intervals(model)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                       lower    est.      upper\n(Intercept)       24.6958881  26.540  28.384112\nTreatmentP        -2.8977028  -0.268   2.361703\nweek1            -14.6767987 -13.018 -11.359201\nweek4            -12.6847987 -11.026  -9.367201\nweek6             -7.4367987  -5.778  -4.119201\nTreatmentP:week1   9.0601043  11.406  13.751896\nTreatmentP:week4   6.4781043   8.824  11.169896\nTreatmentP:week6   0.8061043   3.152   5.497896\n\n Random Effects:\n  Level: ID \n                  lower     est.    upper\nsd((Intercept)) 4.33785 5.112717 6.025997\n\n Within-group standard error:\n   lower     est.    upper \n3.887059 4.214287 4.569062 \n\nnlme::intervals(model2)\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                      lower        est.      upper\n(Intercept)       24.630905  26.3938776  28.156850\nTreatmentP        -2.623490  -0.1218776   2.379735\nweek1            -14.359781 -12.9000000 -11.440219\nweek4            -12.318964 -10.8591837  -9.399403\nweek6             -7.972026  -6.5122449  -5.052464\nTreatmentP:week1   9.233907  11.2880000  13.342093\nTreatmentP:week4   6.603090   8.6571837  10.711277\nTreatmentP:week6   1.832151   3.8862449   5.940338\n\n Random Effects:\n  Level: ID \n                   lower     est.    upper\nsd((Intercept)) 4.334069 5.083126 5.961643\n\n Within-group standard error:\n   lower     est.    upper \n3.384769 3.671238 3.981952 \n\nnlme::fixef(model)-nlme::fixef(model2)\n\n     (Intercept)       TreatmentP            week1            week4 \n       0.1461224       -0.1461224       -0.1180000       -0.1668163 \n           week6 TreatmentP:week1 TreatmentP:week4 TreatmentP:week6 \n       0.7342449        0.1180000        0.1668163       -0.7342449 \n\nanova(model2)\n\n               numDF denDF   F-value p-value\n(Intercept)        1   291 1606.9197  &lt;.0001\nTreatment          1    97   28.8577  &lt;.0001\nweek               3   291   77.0542  &lt;.0001\nTreatment:week     3   291   46.2000  &lt;.0001\n\n\n\n\n1.6.3 Sensitivity analysis - We could have fit a quadratic or piece-wise linear model to the data.\n\nTLC_long_pl=TLC_long_3\nTLC_long_pl$week=as.numeric(as.character(TLC_long_3$week))+1\nTLC_long_pl$time1=(TLC_long_pl$week&lt;2)*TLC_long_pl$week\n# TLC_long_pl$time2=(TLC_long_pl$week&gt;=3)*(TLC_long_pl$week-TLC_long_pl$week)\nhead(TLC_long_pl)\n\n    ID Treatment week    W time1\n1.0  1         P    1 30.8     1\n2.0  2         A    1 26.5     1\n3.0  3         A    1 25.8     1\n4.0  4         P    1 24.7     1\n5.0  5         A    1 20.4     1\n6.0  6         A    1 20.4     1\n\nlattice::xyplot(W ~ week|Treatment,data=TLC_long_pl  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\n\n\nmodel_pl &lt;- nlme::lme(fixed= W ~ week+time1+Treatment+Treatment*week+Treatment*time1,random= ~1|ID, data = TLC_long_pl) #to run the model\nsummary(model_pl)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_pl \n       AIC      BIC    logLik\n  2382.985 2414.714 -1183.492\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.076697 3.706653\n\nFixed effects:  W ~ week + time1 + Treatment + Treatment * week + Treatment *      time1 \n                      Value Std.Error  DF    t-value p-value\n(Intercept)       10.561547 1.0495337 293  10.063085       0\nweek               1.230397 0.1487828 293   8.269756       0\ntime1             14.601933 0.8194318 293  17.819584       0\nTreatmentP        14.507927 1.4768248  97   9.823729       0\nweek:TreatmentP   -1.432713 0.2093560 293  -6.843432       0\ntime1:TreatmentP -13.197091 1.1530427 293 -11.445449       0\n Correlation: \n                 (Intr) week   time1  TrtmnP wk:TrP\nweek             -0.662                            \ntime1            -0.549  0.666                     \nTreatmentP       -0.711  0.470  0.390              \nweek:TreatmentP   0.470 -0.711 -0.473 -0.662       \ntime1:TreatmentP  0.390 -0.473 -0.711 -0.549  0.666\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.39986915 -0.47078045 -0.04895001  0.39625544  3.32467517 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\nsummary(model2)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_3 \n      AIC     BIC    logLik\n  2371.01 2410.62 -1175.505\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.083126 3.671238\n\nFixed effects:  W ~ 1 + Treatment + week + Treatment * week \n                      Value Std.Error  DF    t-value p-value\n(Intercept)       26.393878 0.8957514 291  29.465627  0.0000\nTreatmentP        -0.121878 1.2604340  97  -0.096695  0.9232\nweek1            -12.900000 0.7417021 291 -17.392428  0.0000\nweek4            -10.859184 0.7417021 291 -14.640897  0.0000\nweek6             -6.512245 0.7417021 291  -8.780135  0.0000\nTreatmentP:week1  11.288000 1.0436674 291  10.815707  0.0000\nTreatmentP:week4   8.657184 1.0436674 291   8.294964  0.0000\nTreatmentP:week6   3.886245 1.0436674 291   3.723643  0.0002\n Correlation: \n                 (Intr) TrtmnP week1  week4  week6  TrtP:1 TrtP:4\nTreatmentP       -0.711                                          \nweek1            -0.414  0.294                                   \nweek4            -0.414  0.294  0.500                            \nweek6            -0.414  0.294  0.500  0.500                     \nTreatmentP:week1  0.294 -0.414 -0.711 -0.355 -0.355              \nTreatmentP:week4  0.294 -0.414 -0.355 -0.711 -0.355  0.500       \nTreatmentP:week6  0.294 -0.414 -0.355 -0.355 -0.711  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.63582605 -0.49846350 -0.05679961  0.40026479  3.28119867 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\n\n\n#we can plot the xy plot of the fitted values\n\n\nyhat=predict(model_pl,newdata = TLC_long_pl[,-4],level=0:1)\nTLC_long_4=TLC_long_pl\nTLC_long_4$yhat=yhat[,3]\n\nlattice::xyplot(yhat ~ week|Treatment,data=TLC_long_4  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\nlattice::xyplot(yhat ~ week|Treatment,data=TLC_long_2  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\nlattice::xyplot(W ~ week|Treatment,data=TLC_long_2  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\n\n\n# residuals over time?\n\n# Residuals vs. Fitted (no patterns)\nplot(model_pl, main = \"Plot of residuals vs. fitted.\")\n\n\n\n# QQPlot for normality of errors\nqqnorm(model_pl, ~ residuals(., type=\"pearson\")) # Some issues... probably\n\n\n\n# Plots for the Predicted (BLUPs)\nplot(nlme::ranef(model_pl)) \n\n\n\nqqnorm(model_pl, ~ranef(.)) # These look okay!\n\n\n\n# model$residuals\n\n\n# Observed vs. Fitted\nplot(model_pl, W ~ fitted(.), abline = c(0,1), main = \"Observed vs. Fitted\")\n\n\n\n# Could also look (e.g.) by treatment, if it existed!\n\nresiduals=residuals(model_pl, type=\"pearson\")\nre=nlme::ranef(model_pl)\nre2=rep(re$`(Intercept)` ,each=4)\n\nplot(residuals,re2)\n\n\n\nlength(residuals)\n\n[1] 396\n\nlength(re)\n\n[1] 1\n\n\n\nTLC_long_pl$weeksq= TLC_long_pl$week^2\nmodel_q &lt;- nlme::lme(fixed= W ~ week+weeksq+Treatment+week*Treatment+weeksq*Treatment,random= ~1|ID, data = TLC_long_pl) #to run the model\nsummary(model_q)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_pl \n      AIC      BIC   logLik\n  2485.78 2517.509 -1234.89\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    4.948122 4.346842\n\nFixed effects:  W ~ week + weeksq + Treatment + week * Treatment + weeksq * Treatment \n                     Value Std.Error  DF    t-value p-value\n(Intercept)       32.08886 1.2914204 293  24.847722   0.000\nweek              -9.43993 0.7337490 293 -12.865333   0.000\nweeksq             1.12085 0.0908875 293  12.332330   0.000\nTreatmentP        -5.11030 1.8171896  97  -2.812198   0.006\nweek:TreatmentP    8.33921 1.0324764 293   8.076897   0.000\nweeksq:TreatmentP -1.02915 0.1278901 293  -8.047157   0.000\n Correlation: \n                  (Intr) week   weeksq TrtmnP wk:TrP\nweek              -0.763                            \nweeksq             0.707 -0.984                     \nTreatmentP        -0.711  0.542 -0.502              \nweek:TreatmentP    0.542 -0.711  0.699 -0.763       \nweeksq:TreatmentP -0.502  0.699 -0.711  0.707 -0.984\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.14185550 -0.45450738 -0.04543433  0.47296981  3.35222137 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\n# model_q &lt;- nlme::lme(fixed= W ~ week+weeksq+Treatment+weeksq*Treatment,random= ~1|ID, data = TLC_long_pl) #to run the model\nsummary(model_q)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_pl \n      AIC      BIC   logLik\n  2485.78 2517.509 -1234.89\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    4.948122 4.346842\n\nFixed effects:  W ~ week + weeksq + Treatment + week * Treatment + weeksq * Treatment \n                     Value Std.Error  DF    t-value p-value\n(Intercept)       32.08886 1.2914204 293  24.847722   0.000\nweek              -9.43993 0.7337490 293 -12.865333   0.000\nweeksq             1.12085 0.0908875 293  12.332330   0.000\nTreatmentP        -5.11030 1.8171896  97  -2.812198   0.006\nweek:TreatmentP    8.33921 1.0324764 293   8.076897   0.000\nweeksq:TreatmentP -1.02915 0.1278901 293  -8.047157   0.000\n Correlation: \n                  (Intr) week   weeksq TrtmnP wk:TrP\nweek              -0.763                            \nweeksq             0.707 -0.984                     \nTreatmentP        -0.711  0.542 -0.502              \nweek:TreatmentP    0.542 -0.711  0.699 -0.763       \nweeksq:TreatmentP -0.502  0.699 -0.711  0.707 -0.984\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.14185550 -0.45450738 -0.04543433  0.47296981  3.35222137 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\nsummary(model2)\n\nLinear mixed-effects model fit by REML\n  Data: TLC_long_3 \n      AIC     BIC    logLik\n  2371.01 2410.62 -1175.505\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept) Residual\nStdDev:    5.083126 3.671238\n\nFixed effects:  W ~ 1 + Treatment + week + Treatment * week \n                      Value Std.Error  DF    t-value p-value\n(Intercept)       26.393878 0.8957514 291  29.465627  0.0000\nTreatmentP        -0.121878 1.2604340  97  -0.096695  0.9232\nweek1            -12.900000 0.7417021 291 -17.392428  0.0000\nweek4            -10.859184 0.7417021 291 -14.640897  0.0000\nweek6             -6.512245 0.7417021 291  -8.780135  0.0000\nTreatmentP:week1  11.288000 1.0436674 291  10.815707  0.0000\nTreatmentP:week4   8.657184 1.0436674 291   8.294964  0.0000\nTreatmentP:week6   3.886245 1.0436674 291   3.723643  0.0002\n Correlation: \n                 (Intr) TrtmnP week1  week4  week6  TrtP:1 TrtP:4\nTreatmentP       -0.711                                          \nweek1            -0.414  0.294                                   \nweek4            -0.414  0.294  0.500                            \nweek6            -0.414  0.294  0.500  0.500                     \nTreatmentP:week1  0.294 -0.414 -0.711 -0.355 -0.355              \nTreatmentP:week4  0.294 -0.414 -0.355 -0.711 -0.355  0.500       \nTreatmentP:week6  0.294 -0.414 -0.355 -0.355 -0.711  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.63582605 -0.49846350 -0.05679961  0.40026479  3.28119867 \n\nNumber of Observations: 396\nNumber of Groups: 99 \n\n\n\n#we can plot the xy plot of the fitted values\n\n\nyhat=predict(model_q,newdata = TLC_long_pl[,-4],level=0:1)\nTLC_long_5=TLC_long_pl\nTLC_long_5$yhat=yhat[,3]\n\npar(mfrow=c(2,2))\n\nlattice::xyplot(yhat ~ week|Treatment,data=TLC_long_5  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\nlattice::xyplot(yhat ~ week|Treatment,data=TLC_long_4  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\nlattice::xyplot(yhat ~ week|Treatment,data=TLC_long_2  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\nlattice::xyplot(W ~ week|Treatment,data=TLC_long_2  , groups = ID, \n                col = 'black', \n                type = c('l', 'p'))\n\n\n\n\nConclusions:\n\nAll models indicate a significant effect of treatment, with the largest drop being a time point\n\n\n\nThe lead levels seem to be returning to baseline over time\nThe treatment certainly reduces lead levels for a few weeks\nInvestigate subject data with ID 40.\n\nWe can get more specific too\n\nAt time point 1, individual lead levels seem to drop by 11 points over the placebo.\nAfter that, the gap starts closing over time - valued at 11, 8, and then 3\n\n\n\n\n\nCabrera, Javier, and Andrew McDougall. 2002. Statistical Consulting. Springer New York. https://doi.org/10.1007/978-1-4757-3663-2.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. https://doi.org/10.1017/cbo9780511790942.\n\n\nPinheiro, J. C., and D. Bates. 2009. Mixed-Effects Models in s and s-PLUS. Statistics and Computing. Springer. https://books.google.ca/books?id=y54QDUTmvDcC.\n\n\nWu, Lang. 2019. Mixed Effects Models for Complex Data. Chapman & Hall/CRC Monographs on Statistics & Applied Probability. Philadelphia, PA: Chapman & Hall/CRC."
  },
  {
    "objectID": "glmm.html#methodology-overview",
    "href": "glmm.html#methodology-overview",
    "title": "2  Generalized linear mixed models",
    "section": "2.1 Methodology overview",
    "text": "2.1 Methodology overview\n\n2.1.1 Model structure\nHere, we briefly cover generalized linear mixed models (GLMMs). GLMMs should be used when you have clustered data, and a response variable that is not continuous. They can mainly be fit with the glmm package in R. package in R. You may need other packages for multinomial (mclogit) and cumulative logit (clmm) regression. In general, the GLMM is the child of LMMs and GLMs. Link functions and random effects join forces!\nWe may consider the linear predictor: \\[\\eta=X\\alpha+Z\\beta.\\] The link function allows us to LINK the response \\(Y\\) to \\(\\eta\\). We can then define:\n\n\\(\\eta=X\\alpha+Z\\beta\\)\n\\(g\\) – the link function\n\\(g^{-1}\\) – the inverse link function\n\nWe can the concisely write our model as: \\(g(E(Y))=\\eta\\), \\(E(Y)=g^{-1}(\\eta)\\) and \\(Y=h(\\eta)+\\epsilon\\).\nFor instance, popular link functions include: - logit: \\(g(x)=logit(x)=\\log(x/(1-x))\\) – for binary response - log: \\(g(x)=\\log(x)\\) – for count response - logit: \\(g(x)=x\\) – continuous\n\n\n2.1.2 Interpretations\n\nThe interpretations at the population level follow that of usual GLMs/mixed models. For instance, in logistic mixed regression, you may talk about the probability of a success for different levels of predictors. Or you might say an increase in a predictor causes a certain change in the log odds of a success.\nFor example, you may wish to recall that \\(logit(P(A))=\\log(odds(A))=\\log(P(A)/(1-P(A)))\\) is the log odds of \\(A\\) happening.\n\\(odds(A)/odds(B)\\) is the odds ratio of \\(A\\) to \\(B\\)\n\\(P(A)/P(B)\\) is the relative risk of \\(A\\) to \\(B\\)\n\nFor instance, we may consider \\(OR=odds(male\\ gets\\ disease)/odds(female\\ gets\\ disease)\\). In the logistic model, we have that \\(\\log(odds(male\\ gets\\ disease)/odds(female\\ gets\\ disease))=\\alpha_{gender}\\). The usual interpretation is holding all other variables fixed, in the case of a GLMM, this includes the random effect. Then, \\(\\alpha_{gender}\\) would be the increase in log odds for someone in the same cluster. If there is large variability between clusters, the fixed effects may be comparatively small. It is important to see the effect on the probability.\nLet’s see the impact of different levels of variability on the probability of, say recovery, based off of sex:\n\nexpit=function(x){(1+exp(-x))}\nsimulate_p=function(re_var=1,n=1000){\n\n  \n  sex=rbinom(n,1,1/2)\n  re=rnorm(n,0,re_var)\n  alpha=3\n  odds=expit(sex*alpha+re)\n  p=odds/(1+odds)\n  boxplot(p~sex)\n}\nsimulate_p(0.1)\n\n\n\nsimulate_p(1)\n\n\n\nsimulate_p(10)\n\n\n\nsimulate_p(25)\n\n\n\n\nWe can do something similar for count data. If we consider a Poisson model, with the log link, we have that\n\nsimulate_count=function(re_var=1,n=1000){\n\n  \n  sex=rbinom(n,1,1/2)\n  re=rnorm(n,0,re_var)\n  alpha=log(5)\n  EX_Count=exp(log(10)+re+sex*alpha)\n  boxplot(EX_Count~sex)\n}\nsimulate_count(0.1)\n\n\n\nsimulate_count(0.5)\n\n\n\nsimulate_count(1)\n\n\n\nsimulate_count(2)\n\n\n\n\n\n\n2.1.3 Final notes\n\nThe estimates are usually computed via QMLE, or MCMC methods\nCan take long to fit them\nNeed enough samples at each level"
  },
  {
    "objectID": "glmm.html#case-study-2.1",
    "href": "glmm.html#case-study-2.1",
    "title": "2  Generalized linear mixed models",
    "section": "2.2 Case study 2.1",
    "text": "2.2 Case study 2.1\nIn general, you will have to learn about specific GLMMs individually/case-by-case, so we will proceed with a case study. The following is adapted from: here .\nCase information: What patient and physician factors explain lung cancer remission after treatment?\nSetup:\n\nhdp = read.csv(\"https://stats.idre.ucla.edu/stat/data/hdp.csv\")\nhdp = within(hdp, {\n  Married = factor(Married, levels = 0:1, labels = c(\"no\", \"yes\"))\n  DID = factor(DID)\n  HID = factor(HID)\n  CancerStage = factor(CancerStage)\n})\n\nLet’s explore the data, focusing on :\n\nIl6, CRP: Biological measurements\nLengthofStay\nCancerStage (I, II, III, or IV),\nExperience (doctor)\ndoctor ID - cluster variable\n\n\nhead(hdp)\n\n  tumorsize      co2 pain wound mobility ntumors nmorphine remission\n1  67.98120 1.534333    4     4        2       0         0         0\n2  64.70246 1.676132    2     3        2       0         0         0\n3  51.56700 1.533445    6     3        2       0         0         0\n4  86.43799 1.453300    3     3        2       0         0         0\n5  53.40018 1.566348    3     4        2       0         0         0\n6  51.65727 1.417868    4     5        2       0         0         0\n  lungcapacity      Age Married FamilyHx SmokingHx    Sex CancerStage\n1    0.8010882 64.96824      no       no    former   male          II\n2    0.3264440 53.91714      no       no    former female          II\n3    0.5650309 53.34730     yes       no     never female          II\n4    0.8484109 41.36804      no       no    former   male           I\n5    0.8864910 46.80042      no       no     never   male          II\n6    0.7010307 51.92936     yes       no     never   male           I\n  LengthofStay      WBC      RBC      BMI       IL6       CRP DID Experience\n1            6 6087.649 4.868416 24.14424  3.698981 8.0864168   1         25\n2            6 6700.310 4.679052 29.40516  2.627481 0.8034876   1         25\n3            5 6042.809 5.005862 29.48259 13.896153 4.0341565   1         25\n4            5 7162.697 5.265058 21.55726  3.008033 2.1258629   1         25\n5            6 6443.440 4.984259 29.81519  3.890698 1.3493239   1         25\n6            5 6800.549 5.199714 27.10252  1.418219 2.1946941   1         25\n   School Lawsuits HID  Medicaid\n1 average        3   1 0.6058667\n2 average        3   1 0.6058667\n3 average        3   1 0.6058667\n4 average        3   1 0.6058667\n5 average        3   1 0.6058667\n6 average        3   1 0.6058667\n\nnames(hdp)\n\n [1] \"tumorsize\"    \"co2\"          \"pain\"         \"wound\"        \"mobility\"    \n [6] \"ntumors\"      \"nmorphine\"    \"remission\"    \"lungcapacity\" \"Age\"         \n[11] \"Married\"      \"FamilyHx\"     \"SmokingHx\"    \"Sex\"          \"CancerStage\" \n[16] \"LengthofStay\" \"WBC\"          \"RBC\"          \"BMI\"          \"IL6\"         \n[21] \"CRP\"          \"DID\"          \"Experience\"   \"School\"       \"Lawsuits\"    \n[26] \"HID\"          \"Medicaid\"    \n\n# We can use ggplot2 for this one... \n# This si\nggpairs(hdp[, c(\"IL6\", \"CRP\", \"LengthofStay\", \"Experience\")])\n\n\n\nggplot(hdp, aes(x = CancerStage, y = LengthofStay)) +\n  geom_boxplot()\n\n\n\ntmp = melt(hdp[, c(\"CancerStage\", \"IL6\", \"CRP\")], id.vars=\"CancerStage\")\nggplot(tmp, aes(x = CancerStage, y = value)) +\n  geom_boxplot() +\n  facet_grid(variable ~ .) \n\n\n\nggplot(tmp, aes(x = CancerStage, y = value)) +\n  geom_boxplot() +\n  facet_grid(variable ~ .) +\n  scale_y_sqrt()\n\n\n\ntmp = melt(hdp[, c(\"remission\", \"IL6\", \"CRP\", \"LengthofStay\", \"Experience\")],\n  id.vars=\"remission\")\nggplot(tmp, aes(factor(remission), y = value, fill=factor(remission))) +\n  geom_boxplot() +\n  facet_wrap(~variable, scales=\"free_y\")\n\n\n\n\n\n2.2.1 Fitting the model\n\n# mixed effects logistic regression model\n# estimate the model\n# model = glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +\n    # (1 | DID), data = hdp, family = binomial)\n\nmodel = glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +\n    (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer = \"bobyqa\"),nAGQ = 10)\n\n\n# print the mod results without correlations among fixed effects\nprint(model, corr = FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]\n Family: binomial  ( logit )\nFormula: remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +  \n    (1 | DID)\n   Data: hdp\n      AIC       BIC    logLik  deviance  df.resid \n 7397.276  7460.733 -3689.638  7379.276      8516 \nRandom effects:\n Groups Name        Std.Dev.\n DID    (Intercept) 2.015   \nNumber of obs: 8525, groups:  DID, 407\nFixed Effects:\n   (Intercept)             IL6             CRP   CancerStageII  CancerStageIII  \n      -2.05269        -0.05677        -0.02148        -0.41393        -1.00346  \n CancerStageIV    LengthofStay      Experience  \n      -2.33704        -0.12118         0.12009  \n\n# broom::tidy(model)\n\n\nse = sqrt(diag(vcov(model)))\n# table of estimates with 95% CI\ntab = cbind(Est = fixef(model), LL = fixef(model) - 1.96 * se, UL = fixef(model) + 1.96 *se); tab\n\n                       Est          LL           UL\n(Intercept)    -2.05269401 -3.09430566 -1.011082369\nIL6            -0.05677185 -0.07934786 -0.034195848\nCRP            -0.02148294 -0.04151100 -0.001454893\nCancerStageII  -0.41393409 -0.56243123 -0.265436940\nCancerStageIII -1.00346486 -1.19609923 -0.810830493\nCancerStageIV  -2.33703717 -2.64682992 -2.027244418\nLengthofStay   -0.12118214 -0.18710336 -0.055260913\nExperience      0.12008835  0.06628404  0.173892667\n\n#Odds ratios\nexp(tab)\n\n                      Est         LL        UL\n(Intercept)    0.12838856 0.04530646 0.3638250\nIL6            0.94480960 0.92371854 0.9663822\nCRP            0.97874617 0.95933879 0.9985462\nCancerStageII  0.66104452 0.56982201 0.7668708\nCancerStageIII 0.36660700 0.30237140 0.4444888\nCancerStageIV  0.09661346 0.07087554 0.1316979\nLengthofStay   0.88587259 0.82935801 0.9462382\nExperience     1.12759647 1.06853018 1.1899278\n\n\nInference - use normal approximation, or bootstrapping… For boostrapping, we need to sample one level at a time.\n\n#resamples single level clustered data\nsampler = function(dat, clustervar, replace = TRUE, reps = 1) {\n    # Unique clusters\n    cid = unique(dat[, clustervar[1]])\n    # num clusters\n    ncid = length(cid)\n    # sampled clusters for each rep\n    recid = sample(cid, size = ncid * reps, replace = TRUE)\n    if (replace) {\n      # This line is grabbing all the rows corresponding to each cluster, sampling them, and assigning a new id\n        rid = lapply(seq_along(recid), function(i) {\n            cbind(NewID = i, RowID = sample(which(dat[, clustervar] == recid[i]),\n                size = length(which(dat[, clustervar] == recid[i])), replace = TRUE))\n        })\n    } \n    \n    else {\n      # This line is grabbing all the rows corresponding to each cluster and assigning a new id\n        rid = lapply(seq_along(recid), function(i) {\n            cbind(NewID = i, RowID = which(dat[, clustervar] == recid[i]))\n        })\n    }\n    #put the above info in a dataframe\n    dat = as.data.frame(do.call(rbind, rid))\n    \n    # put the above info in a dataframe, cut divides the above long samples into the replicate samples. include.lowest just includes the left side of the interval, but not the right\n    dat$Replicate = cut(dat$NewID, breaks = c(1, ncid * 1:reps), include.lowest = TRUE,labels = FALSE)%&gt;%factor()\n    # change to factor\n    dat$NewID = factor(dat$NewID)\n    return(dat)\n}\n\n\nhead(sampler(hdp, \"DID\", reps = 1))\n\n  NewID RowID Replicate\n1     1  6148         1\n2     1  6149         1\n3     1  6156         1\n4     1  6150         1\n5     1  6146         1\n6     1  6157         1\n\n\n\nset.seed(1241)\n#sample 100 samples, use more in reality\n\ntmp = sampler(hdp, \"DID\", reps = 100)\n\n\nbigdata = cbind(tmp, hdp[tmp$RowID, ])\nhead(bigdata)\n\n     NewID RowID Replicate tumorsize      co2 pain wound mobility ntumors\n1137     1  1137         1  61.60908 1.385558    4     5        5       4\n1131     1  1131         1  62.90957 1.489077    3     5        4       1\n1158     1  1158         1  61.15147 1.694328    5     6        4       0\n1151     1  1151         1  94.27969 1.752081    5     7        5       2\n1152     1  1152         1  85.51482 1.532822    3     6        6       6\n1148     1  1148         1  73.29905 1.770178    5     6        6       5\n     nmorphine remission lungcapacity      Age Married FamilyHx SmokingHx\n1137         7         0    0.9750009 55.83614      no       no     never\n1131         4         0    0.9268863 51.04217     yes       no     never\n1158         0         0    0.6443683 43.13214     yes       no     never\n1151         7         0    0.8582457 62.36135     yes       no   current\n1152         5         0    0.4208578 61.63105      no      yes     never\n1148         4         0    0.9638132 49.00940      no       no   current\n        Sex CancerStage LengthofStay      WBC      RBC      BMI       IL6\n1137 female          II            6 4967.423 5.412072 29.58874 6.1558399\n1131 female          II            6 5605.937 4.793538 23.66554 3.7553295\n1158 female           I            5 5483.016 5.040751 28.64414 2.5412189\n1151   male           I            5 6337.338 6.064870 30.33128 3.2348010\n1152 female          IV            7 4039.674 5.266622 25.28456 0.7059072\n1148   male           I            4 5365.597 4.970115 39.41294 0.7627430\n            CRP DID Experience  School Lawsuits HID  Medicaid\n1137  1.9068986  52         19 average        3   5 0.2188103\n1131  0.6346611  52         19 average        3   5 0.2188103\n1158  3.7357859  52         19 average        3   5 0.2188103\n1151  8.2530591  52         19 average        3   5 0.2188103\n1152  4.4492342  52         19 average        3   5 0.2188103\n1148 10.7412680  52         19 average        3   5 0.2188103\n\nf = fixef(model); f\n\n   (Intercept)            IL6            CRP  CancerStageII CancerStageIII \n   -2.05269401    -0.05677185    -0.02148294    -0.41393409    -1.00346486 \n CancerStageIV   LengthofStay     Experience \n   -2.33703717    -0.12118214     0.12008835 \n\nr = getME(model, \"theta\"); r\n\nDID.(Intercept) \n       2.014552 \n\n\n\n# Make cluster\ncl = makeCluster(10)\nclusterExport(cl, c(\"bigdata\", \"f\", \"r\"))\na=clusterEvalQ(cl, require(lme4))\n\n\nmyboot = function(i) {\n  # fit the mdoel for every bootstrap sample\n    object = try(glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay +\n        Experience + (1 | NewID), data = bigdata, subset = Replicate == i, family = binomial,\n        nAGQ = 1, start = list(fixef = f, theta = r)), silent = TRUE)\n    if (class(object) == \"try-error\")\n        return(object)\n    c(fixef(object), getME(object, \"theta\"))\n}\n\n\nstart = proc.time()\nres = parLapplyLB(cl, X = levels(bigdata$Replicate), fun = myboot)\nend = proc.time()\n# shut down the cluster\nstopCluster(cl)\n\n\n#Check how many models converge\nsuccess = sapply(res, is.numeric)\nmean(success)\n\n# combine successful results\nbigres = do.call(cbind, res[success])\n\n# calculate 2.5th and 97.5th percentiles for 95% CI\n(ci = t(apply(bigres, 1, quantile, probs = c(0.025, 0.975))))\n\n# All results\nfinaltable = cbind(Est = c(f, r), SE = c(se, NA), BootMean = rowMeans(bigres),ci)\n# round and print\nround(finaltable, 3)\n\n\n\n2.2.2 Predicted probabilities and graphing\nWe may wish to plot some of the fitted functions/probabilities.\nNow we can look at by length of stay:\n\n# temporary data\ntmpdat = hdp[, c(\"IL6\", \"CRP\", \"CancerStage\", \"LengthofStay\", \"Experience\",\"DID\")]\n\nsummary(hdp$LengthofStay)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   5.000   5.000   5.492   6.000  10.000 \n\njvalues =  seq(from = min(hdp$LengthofStay), to = max(hdp$LengthofStay), l = 100)\n\n# calculate predicted probabilities and store in a list\npp = lapply(jvalues, function(j) {\n    tmpdat$LengthofStay = j\n    predict(model, newdata = tmpdat, type = \"response\")\n})\n\n# average marginal predicted probability across a few different Lengths of\n# Stay\njvalues[c(1, 20, 40, 60, 80, 100)]\n\n[1]  1.000000  2.727273  4.545455  6.363636  8.181818 10.000000\n\nsapply(pp[c(1, 20, 40, 60, 80, 100)], mean)\n\n[1] 0.3652319 0.3366360 0.3075494 0.2796359 0.2530109 0.2277694\n\n# get the means with lower and upper quartiles\nplotdat = t(sapply(pp, function(x) {\n    c(M = mean(x), quantile(x, c(0.25, 0.75)))\n}))\nhead(plotdat)\n\n             M        25%       75%\n[1,] 0.3652319 0.08489874 0.6155638\n[2,] 0.3637056 0.08404676 0.6129535\n[3,] 0.3621815 0.08320255 0.6103367\n[4,] 0.3606597 0.08236605 0.6077135\n[5,] 0.3591402 0.08153722 0.6050841\n[6,] 0.3576230 0.08071600 0.6024486\n\n# add in LengthofStay values and convert to data frame\nplotdat = as.data.frame(cbind(plotdat, jvalues))\n\n# better names and show the first few rows\ncolnames(plotdat) = c(\"PredictedProbability\", \"Lower\", \"Upper\", \"LengthofStay\")\nhead(plotdat)\n\n  PredictedProbability      Lower     Upper LengthofStay\n1            0.3652319 0.08489874 0.6155638     1.000000\n2            0.3637056 0.08404676 0.6129535     1.090909\n3            0.3621815 0.08320255 0.6103367     1.181818\n4            0.3606597 0.08236605 0.6077135     1.272727\n5            0.3591402 0.08153722 0.6050841     1.363636\n6            0.3576230 0.08071600 0.6024486     1.454545\n\n# plot average marginal predicted probabilities\nggplot(plotdat, aes(x = LengthofStay, y = PredictedProbability)) + geom_line(linewidth = 2) +\n    ylim(c(0, 1))\n\n\n\nggplot(plotdat, aes(x = LengthofStay, y = PredictedProbability)) + geom_linerange(aes(ymin = Lower,\n    ymax = Upper)) + geom_line(linewidth = 2) + ylim(c(0, 1))\n\n\n\n#################\n\nNow we can look at by stage:\n\n# calculate predicted probabilities for each stage and store in a list\nbiprobs = lapply(levels(hdp$CancerStage), function(stage) {\n  tmpdat$CancerStage = stage\n  lapply(jvalues, function(j) {\n    tmpdat$LengthofStay = j\n    predict(model, newdata = tmpdat, type = \"response\")\n  })\n})\n\n# get means and quartiles for all jvalues for each level of CancerStage\nplotdat2 = lapply(biprobs, function(X) {\n  temp = t(sapply(X, function(x) {\n    c(M=mean(x), quantile(x, c(.25, .75)))\n  }))\n  temp = as.data.frame(cbind(temp, jvalues))\n  colnames(temp) = c(\"PredictedProbability\", \"Lower\", \"Upper\", \"LengthofStay\")\n  return(temp)\n})\n\n\n\n# collapse to one data frame\nplotdat2 = do.call(rbind, plotdat2)\n\n# add cancer stage\nplotdat2$CancerStage = factor(rep(levels(hdp$CancerStage), each = length(jvalues)))\n\n# show first few rows\nhead(plotdat2)\n\n  PredictedProbability     Lower     Upper LengthofStay CancerStage\n1            0.4474662 0.1547407 0.7328360     1.000000           I\n2            0.4458001 0.1533052 0.7306736     1.090909           I\n3            0.4441352 0.1518807 0.7285001     1.181818           I\n4            0.4424716 0.1504671 0.7263157     1.272727           I\n5            0.4408092 0.1490643 0.7241204     1.363636           I\n6            0.4391481 0.1476723 0.7219142     1.454545           I\n\n# graph it\nggplot(plotdat2, aes(x = LengthofStay, y = PredictedProbability)) +\n  geom_ribbon(aes(ymin = Lower, ymax = Upper, fill = CancerStage), alpha = .15) +\n  geom_line(aes(colour = CancerStage), size = 2) +\n  ylim(c(0, 1)) + facet_wrap(~  CancerStage)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nggplot(data.frame(Probs = biprobs[[4]][[100]]), aes(Probs)) + geom_histogram() +\n    scale_x_sqrt(breaks = c(0.01, 0.1, 0.25, 0.5, 0.75))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe may have considered the variation within hospital as well… Or toher variables etc.\n\n# estimate the model and store results in m\nm3a = glmer(remission ~ Age + LengthofStay + FamilyHx + IL6 + CRP +\n  CancerStage + Experience + (1 | DID) + (1 | HID),\n  data = hdp, family = binomial, nAGQ=1)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.393634 (tol = 0.002, component 1)\n\n\n\n# print the mod results without correlations among fixed effects\nprint(m3a, corr=FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: remission ~ Age + LengthofStay + FamilyHx + IL6 + CRP + CancerStage +  \n    Experience + (1 | DID) + (1 | HID)\n   Data: hdp\n      AIC       BIC    logLik  deviance  df.resid \n 7199.081  7283.690 -3587.541  7175.081      8513 \nRandom effects:\n Groups Name        Std.Dev.\n DID    (Intercept) 1.9513  \n HID    (Intercept) 0.5432  \nNumber of obs: 8525, groups:  DID, 407; HID, 35\nFixed Effects:\n   (Intercept)             Age    LengthofStay     FamilyHxyes             IL6  \n      -1.68299        -0.01496        -0.04577        -1.30789        -0.05729  \n           CRP   CancerStageII  CancerStageIII   CancerStageIV      Experience  \n      -0.02209        -0.31739        -0.85462        -2.13138         0.12703  \noptimizer (Nelder_Mead) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\nhist(ranef(m3a, which = \"DID\")[[1]][,1])\n\n\n\nhist(ranef(m3a, which = \"HID\")[[1]][,1])\n\n\n\n# lattice::dotplot(ranef(m3a, which = \"DID\", condVar = TRUE), scales = list(y = list(alternating = 0)))\n# lattice::dotplot(ranef(m3a, which = \"HID\", condVar = TRUE))\n\nHomework: Try this other model. What is the dotplot saying?\n\n# estimate the model and store results in m\nm3b = glmer(remission ~ Age + LengthofStay + FamilyHx + IL6 + CRP + CancerStage +\n    Experience + (1 + LengthofStay | DID) + (1 | HID), data = hdp, family = binomial,\n    nAGQ = 1)\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 1.66877 (tol = 0.002, component 1)\n\nprint(m3b, corr = FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: remission ~ Age + LengthofStay + FamilyHx + IL6 + CRP + CancerStage +  \n    Experience + (1 + LengthofStay | DID) + (1 | HID)\n   Data: hdp\n      AIC       BIC    logLik  deviance  df.resid \n 7147.749  7246.460 -3559.875  7119.749      8511 \nRandom effects:\n Groups Name         Std.Dev. Corr \n DID    (Intercept)  0.5029        \n        LengthofStay 0.3729   -0.12\n HID    (Intercept)  0.7318        \nNumber of obs: 8525, groups:  DID, 407; HID, 35\nFixed Effects:\n   (Intercept)             Age    LengthofStay     FamilyHxyes             IL6  \n      -0.53725        -0.01523        -0.19062        -1.33822        -0.05865  \n           CRP   CancerStageII  CancerStageIII   CancerStageIV      Experience  \n      -0.02095        -0.29471        -0.86500        -2.30039         0.10412  \noptimizer (Nelder_Mead) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\nlattice::dotplot(ranef(m3b, which = \"DID\", condVar = TRUE), scales = list(y = list(alternating = 0)))\n\n$DID\n\n\n\n\nlattice::dotplot(ranef(m3b, which = \"HID\", condVar = TRUE), scales = list(y = list(alternating = 0)))\n\n$HID"
  },
  {
    "objectID": "permutation_tests.html#introduction",
    "href": "permutation_tests.html#introduction",
    "title": "3  Permutation Tests",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nHere, we briefly cover permutation tests. These are a class of nonparametric hypothesis tests for checking equality of distributions. Let’s start with an example.\nSuppose we sample \\(n\\) observations from \\(F\\), denoted \\((X_{1},\\ldots,X_n)\\) and \\(m\\) from \\(G\\), denoted \\((X_{n+1},\\ldots,X_{n+m})\\). Then, we wish to test \\[H_0 : F=G \\qquad vs.  \\qquad H_1: F\\neq G.\\] Suppose we have some test statistic which is a measure of a difference between groups, for example, the Kolmogrov-Smirnoff statistic: \\[T^*= T(\\ (X_{1},\\ldots,X_n),\\ (X_{n+1},\\ldots,X_{n+m})\\ )=||F_m-G_n||.\\]\nHere, \\(T\\) measures the distance between the empirical distributions from each sample. Recall then that if we want to do a hypothesis test, we need to compute the distribution of \\(T\\) under the null hypothesis.\nUnder the null hypothesis, we have that \\((X_{1},\\ldots,X_n)\\) is equal in distribution to \\((X_{n+1},\\ldots,X_{n+m})\\). This means that the labels 1,…,\\(n+m\\) are arbitrary, in the sense that all observations come from the same population, so the division into the two groups holds no meaning. Furthermore, for any permutation \\(\\pi_1,\\ldots,\\pi_{n+m}\\), it holds that \\((X_{\\pi_1},\\ldots,X_{\\pi_n})\\) is equal in distribution to \\((X_{\\pi_{n+1}},\\ldots,X_{\\pi_{n+m}})\\). This implies that \\(T(\\ (X_{1},\\ldots,X_n),\\ (X_{n+1},\\ldots,X_{n+m})\\ )\\) is equal in distribution to \\(T(\\ (X_{\\pi_1},\\ldots,X_{\\pi_n}),\\ (X_{\\pi_{n+1}},\\ldots,X_{\\pi_{n+m}})\\ )\\). This fact motivates the following procedure:\n\nDraw a random permutation \\(\\pi\\) from the set of \\((n+m)!\\) permutations\nCompute \\(T(\\ (X_{\\pi_1},\\ldots,X_{\\pi_n}),\\ (X_{\\pi_{n+1}},\\ldots,X_{\\pi_{n+m}})\\ )\\)\nRepeat 1 and 2 \\(K\\) times to obtain \\(K\\) values \\(T_1,...,T_K\\)\nLet \\(\\mu_K\\) be the empirical distribution of \\(T^*,T_1,...,T_K\\)\nCompute the p-value: \\(\\Pr_{\\mu_K}(X\\geq T^*)\\)\n\nThe above is a two-sample permutation test! Let’s add some rigor."
  },
  {
    "objectID": "permutation_tests.html#the-permutation-lemma",
    "href": "permutation_tests.html#the-permutation-lemma",
    "title": "3  Permutation Tests",
    "section": "3.2 The Permutation Lemma",
    "text": "3.2 The Permutation Lemma\nConsider again, \\(X_{1},\\ldots,X_{n}\\sim F\\) and \\(X_{n+1},\\ldots,X_{n+m}\\sim F\\). Denote by \\(Y_1,\\ldots,Y_{n+m}\\) the order statistics of the combined sample \\(X_{1},\\ldots,X_{n+m}\\). Now, let \\(V_i=1(Y_i\\in \\{X_{1},\\ldots,X_{m}\\})\\). \nNow, note that \\(Y_1,\\ldots,Y_{n+m}=X_{\\pi_1},\\ldots,X_{\\pi_n},X_{\\pi_{n+1}},\\ldots,X_{\\pi_{n+m}}\\) for some permutation \\(\\pi\\). Furthermore, \\(\\pi\\) has to be uniformly distributed across all possible permutations, on account of the \\(X_i\\) being iid. Therefore, the probability of a given permutation generated by the order statistics is \\(1/(n+m)!\\).\nNext, note each permutation generates a \\((V_1,\\ldots,V_{n+m})\\). Now, the probability of observing any \\((V_1,\\ldots,V_{n+m})\\) is the number of permutations such that the same \\(m\\) elements are in the first sample. This is given by \\(n!m!\\). Given that the permutations are uniformly distributed, we have that the probability that \\((V_1,\\ldots,V_{n+m})=(v_1,\\ldots,v_{n+m})=n!m!/(n+m)!\\). Therefore, under the null hypothesis, every permutation is equally likely and so the distribution of the vector \\((V_1,\\ldots,V_n)\\) is uniform over \\(\\binom{n+m}{m}\\). This has been referred to as the permutation lemma."
  },
  {
    "objectID": "permutation_tests.html#adding-in-t",
    "href": "permutation_tests.html#adding-in-t",
    "title": "3  Permutation Tests",
    "section": "3.3 Adding in \\(T\\)",
    "text": "3.3 Adding in \\(T\\)\nNow, we may ask where \\(T\\) comes in. Let \\(\\mathbf{X}_1=(X_{1},\\ldots,X_n)\\) and \\(\\mathbf{X}_2=(X_{n+1},\\ldots,X_{n+m})\\). Let \\(\\mathbf{Y}\\) be the combined sample order statistics and \\(\\mathbf{V}\\) be the label indicators. We can write:\n\\[\\begin{align*}\n\\Pr(T(\\mathbf{X}_1,\\mathbf{X}_2)\\geq t|\\mathbf{Y})&=\\Pr(T(\\mathbf{V},\\mathbf{Y})\\geq t|\\mathbf{Y})\\\\\n&=\\sum_{i=1}^{\\binom{n+m}{m}}\\Pr(T(v,\\mathbf{Y})\\geq t,\\mathbf{V}=v|\\mathbf{Y})\\\\\n&=\\sum_{i=1}^{\\binom{n+m}{m}}1(T(v,\\mathbf{Y})\\geq t)\\Pr(\\mathbf{V}=v|\\mathbf{Y})\\\\\n&=\\#\\text{ labellings such that } T(v,\\mathbf{Y})\\geq t\\big / \\binom{n+m}{m} .\n\\end{align*}\\]\nThis probability is: the probability that we observe a value for our statistic at least as extreme as, assuming that the null hypothesis is true, given the set of values that we have observed. Now then, \\[p=\\#\\text{ labellings such that } T(v,\\mathbf{Y})\\geq t)\\big / \\binom{n+m}{m}\\] is the p-value, conditional on the combined sample order statistics we observed."
  },
  {
    "objectID": "permutation_tests.html#an-example",
    "href": "permutation_tests.html#an-example",
    "title": "3  Permutation Tests",
    "section": "3.4 An example",
    "text": "3.4 An example\n\nset.seed(8235)\nn=10\nY=rnorm(n)\nX=rnorm(n,2)\n\nTS=ks.test(X,Y)$statistic\n\n\npermutation_test=function(X,Y,test_stat=function(x,y){ks.test(x,y)$statistic},K=1000){\n  \n  combined=c(X,Y)\n  m=length(X)\n  n=length(c(X,Y))\n  permutations=replicate(K,sample(1:n,n))\n  com_Tstar=function(permutation){test_stat(combined[permutation[1:m]],combined[permutation[-(1:m)]])}\n  Ts=apply(permutations,2,com_Tstar)\n  Tss=test_stat(X,Y)\n  p=mean(Tss&lt;=c(Tss,Ts))\n  return(p)\n}\n\npermutation_test(X,Y)\n\n[1] 0.000999001\n\n# How many samples do we need?\n\n\n\n\nsimulate_test=function(){\n  Y=rnorm(n)\n  X=rnorm(n,2)\n  permutation_test(X,Y)}\n\n\nmean(replicate(100,simulate_test())&lt;= 0.05)\n\n[1] 0.88\n\nsimulate_t_test=function(){\n  Y=rnorm(n)\n  X=rnorm(n,2)\n  t.test(X,Y)$p.value}\n\nmean(replicate(100,simulate_t_test())&lt;= 0.05)\n\n[1] 0.99\n\n# A second exmaple\n\nsimulate_test=function(){\n  Y=rexp(n)\n  X=rexp(n,2)\n  permutation_test(X,Y)}\n\nsimulate_test_2=function(){\n  Y=rexp(n)\n  X=rexp(n,2)\n  permutation_test(X,Y,function(x,y){t.test(x,y)$statistic})\n  }\n\nsimulate_t_test=function(){\n  Y=rexp(n)\n  X=rexp(n,2)\n  t.test(X,Y)$p.value}\n\nmean(replicate(50,simulate_test())&lt;= 0.05)\n\n[1] 0.08\n\nmean(replicate(50,simulate_test_2())&lt;= 0.05)\n\n[1] 0\n\nmean(replicate(50,simulate_t_test())&lt;= 0.05)\n\n[1] 0.24\n\n\nA second example – heavy tails:\n\nset.seed(8235)\nn=35\n\n\nsimulate_test=function(){\n  Y=rnorm(n)\n  X=rt(n,3)\n  permutation_test(X,Y)}\n\nsimulate_t_test=function(){\n  Y=rnorm(n)\n  X=rt(n,3)\n  t.test(X,Y)$p.value}\n\nmean(replicate(100,simulate_test())&lt;= 0.05)\n\n[1] 0.06\n\nmean(replicate(100,simulate_t_test())&lt;= 0.05)\n\n[1] 0.07\n\n\nNotes:\n\nWe should take \\(K\\) as large as feasible. 1000 is a rule of thumb.\nThe test statistic chosen has a large impact on the power. It is important to choose a test statistic that will perform well for the problem at hand. For instance, does it need to be robust, efficient computationally? What distributional differences are we most concerned about?\nOf course in simple problems, they will have lower power than optimal tests. However, they are suitable for situations where an optimal test is not easily derived, or the sample size is too low for asymptotic approximations. They are also easy to implement and relatively intuitive (you don’t need to understand the CLT.)\nPermutation tests are mathematically valid because the data are exchangeable under the null hypothesis. We have to be careful that this is directly implied by our assumptions and null hypothesis. For instance, in the setup from the introduction, the null hypothesis $ E_F(X)=E_G(X)$ is not enough to give exchangeability of the data, since we have only assumed the data come from \\(F\\) and \\(G\\), and it could be that \\(F\\neq G\\) but \\(E_F(X)=E_G(X)\\). In that case, assuming \\(H_0\\) alone is not enough to imply that \\((X_{1},\\ldots,X_n)\\) is equal in distribution to \\((X_{n+1},\\ldots,X_{n+m})\\). However, if in addition, we assume that \\(F\\) and \\(G\\) are in the family of normal distributions with variance 1, then we have that \\((X_{1},\\ldots,X_n)\\) is equal in distribution to \\((X_{n+1},\\ldots,X_{n+m})\\)."
  },
  {
    "objectID": "permutation_tests.html#permutation-test-for-independence",
    "href": "permutation_tests.html#permutation-test-for-independence",
    "title": "3  Permutation Tests",
    "section": "3.5 Permutation test for independence",
    "text": "3.5 Permutation test for independence\nSuppose that instead we observe \\(((X_1,Y_1),\\ldots,(X_n,Y_n))\\sim F_{XY}\\), where \\(X_i\\sim F_X\\) and \\(Y_i\\sim F_Y\\). Suppose that we wish to test if \\(X_i\\) are independent of \\(Y_i\\). One way to phrase this is \\[H_0 : F_{XY}=F_XF_Y \\qquad vs. \\qquad H_1: F_{XY}\\neq F_XF_Y.\\] Now, under the null hypothesis, by definition, conditioning on \\(X_i\\) tells us nothing about the distribution of \\(Y_i\\). Therefore, \\(((X_1,Y_1),\\ldots,(X_n,Y_n))\\) is equal in distribution to \\(((X_1,Y_{\\pi_1}),\\ldots,(X_n,Y_{\\pi_n}))\\) for any permuation \\(\\pi\\). Then, still under the null hypothesis, the pairings we observed were arbritrary. In fact, it is easy to see that the pairings are uniformly distributed accross the permutations of the \\(Y_i\\)s. In this case, we draw many ``permutation samples’’ \\(((X_1,Y_{\\pi_1}),\\ldots,(X_n,Y_{\\pi_n}))\\), and compute some statistic \\(T\\) which measures the dependence between \\(X\\) and \\(Y\\). For instance, we may use \\[\\sup_{(x_1,x_2)\\in\\mathbb{R}^2}|F_{XY}((x_1,x_2))-F_X(x_1)F_Y(x_2)|.\\] This test is implemented in the robusTest package.\n\nmv.ks.statistic = function(X, Y) {\n  n = length(X)\n  \n  # The matrix under the assumption of independence is simply the \n  # product of [i/n][j/n] for the (i,j)-th entry of the matrix\n  indep_mat = as.matrix((1:n)/n) %*% t(1:n/n)\n  \n  # Return the maximum difference\n  max(abs(robusTest::ecdf2D(X, Y)$ecdf - indep_mat))\n}\n\nset.seed(31415) \n\n# Generate X and Y dependent; X and W independent\nK = 10000\nn = 100\nX = rnorm(n)\nY = 4*X + rnorm(n, 0, 3)\nW = rexp(n)\n\n# Compute the statistics\nt1 = mv.ks.statistic(X, Y)\nt2 = mv.ks.statistic(X, W)\n\nresults_mat = matrix(nrow = K, ncol = 2)\n\nfor(ii in 1:K) {\n  Xs = X[sample(1:n, n)]\n  results_mat[ii, ] = c(mv.ks.statistic(Xs, Y),\n                         mv.ks.statistic(Xs, W))\n}\n\n# Plot the results\nmatplot(y = results_mat, \n        x = matrix(c(rep(1, nrow(results_mat)),\n                     rep(2, nrow(results_mat))),\n                   nrow = nrow(results_mat)),\n        pch = 19,\n        ylab = \"Estimated Value\",\n        xaxt = 'n', \n        xlim = c(0, 3),\n        ylim = c(0, max(c(results_mat, t1, t2))),\n        xlab = 'Test (#1 and #2)')\n\n\nabline(h = t1, lty = 3, col = 'black')\nabline(h = t2, lty = 3, col = 'red')\n\n\n\n\nWe could just directly use the function: robusTest::indeptest(X,Y).\nFor more information, see these notes."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Cabrera, Javier, and Andrew McDougall. 2002. Statistical\nConsulting. Springer New York. https://doi.org/10.1007/978-1-4757-3663-2.\n\n\nGelman, Andrew, and Jennifer Hill. 2006. Data Analysis Using\nRegression and Multilevel/Hierarchical Models. Cambridge University\nPress. https://doi.org/10.1017/cbo9780511790942.\n\n\nPinheiro, J. C., and D. Bates. 2009. Mixed-Effects Models in s and\ns-PLUS. Statistics and Computing. Springer. https://books.google.ca/books?id=y54QDUTmvDcC.\n\n\nWu, Lang. 2019. Mixed Effects Models for Complex Data. Chapman\n& Hall/CRC Monographs on Statistics & Applied Probability.\nPhiladelphia, PA: Chapman & Hall/CRC."
  }
]